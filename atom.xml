<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RSarXiv</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://rsarxiv.github.io/"/>
  <updated>2016-06-12T19:44:57.000Z</updated>
  <id>http://rsarxiv.github.io/</id>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>当我们在谈论deep learning的时候，我们在谈论什么？</title>
    <link href="http://rsarxiv.github.io/2016/06/12/%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E8%B0%88%E8%AE%BAdeep-learning%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E5%9C%A8%E8%B0%88%E8%AE%BA%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <id>http://rsarxiv.github.io/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/</id>
    <published>2016-06-12T17:59:52.000Z</published>
    <updated>2016-06-12T19:44:57.000Z</updated>
    
    <content type="html">&lt;p&gt;标题起的有一点装x了，昨天看到微博上刘知远老师对关于deep learning哪年火的问题的讨论，突然有一些自己的感触就写了下来。&lt;/p&gt;
&lt;p&gt;从接触nlp到现在大概过去了4个月的时间，最初的动机是要用word2vec工具包来给自己写的app(rsarxiv)添加一个paper knowledge graph的功能。当时用word2vec的感受时，参数太多了，不知道这些参数到底是什么意思，所以就想着看看paper，看看源代码来试着理解下每个参数到底起什么作用，以方便我使用它。就是从这个时候开始算是接触了nlp。&lt;/p&gt;
&lt;p&gt;因为觉得自动文摘是一个非常炫酷的功能，同时也是想给自己的app添加一个根据查询结果自动生成文献综述的功能，所以开始看一些自动文摘方面的paper，正好微博上找到了一个paper list，里面列出了近两年abstractive summarization相关的paper，加上自己买的一些书中介绍了很多传统的extractive的方法，经过了一个月时间的学习，自己从这些资料中学到了很多关于自动文摘的东西，为了记录下所学到的东西和理解到的东西，就写了一个系列博客——&lt;a href=&quot;http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/&quot;&gt;自动文摘&lt;/a&gt;。在看abstractive思路的时候，接触到了当前研究的一个热门方法，seq2seq+attention，这个组合几乎席卷了nlp的所有任务，一次又一次地刷新着排行榜。seq2seq的思路其实并不复杂，没有太多晦涩难懂的数学公式，看着模型图和下面的解释基本就能看懂思路是怎样的，attention也没有多么难，只是在seq2seq的接触上将输出与输入之间的关系考虑更加全面了，而不是简单地认为输出的每一个部分都与整个输入有关，而应该是将注意力放在相关的输入上。&lt;/p&gt;
&lt;p&gt;因为看自动文摘的paper比较过瘾，就想着可以不可以做一个公众号来督促自己每天读一篇paper，写一篇博客。这样既可以养成一个读paper的好习惯，又可以提高自己的写作水平，同时也能够分享给可能感兴趣的童鞋。于是乎开始了PaperWeekly这个side project，前一天晚上睡觉前读paper，理解了其中模型的思路，第二天早上早起，开始写作，希望用尽量短的话介绍清楚该篇paper的模型思路和贡献。关于公众号如何做的问题，和另外一个账号的作者讨论过。他认为发布频率不应该太高，一天一篇太高了，而且要写长文，写干活，写高质量的东西，这样才能提供给用户最好的服务。我觉得他的话没有一点瑕疵，但我做PaperWeekly的初衷还是以自己为主，希望自己每天可以读一篇，写一篇，用最概括的话、图来讲清楚paper的贡献，当然不排除有的用户不喜欢这种方式，那么我只能说您不适合我这个公众号，我也不会因此去改变。于是，这个side project坚持了一个多月，现在有文章22篇+自动文章8篇一共30篇。当然读过的paper不止这些，30篇是写下来的。&lt;code&gt;今后的形式可能是这样，工作日时间较短，所以写单篇，周末的话，时间充裕，写综述，所以每周末都会将下周的5篇文章选好，尽量是相似topic的，这样方便写综述。&lt;/code&gt;我觉得这是一个好习惯，长此以往坚持下来，一定会有很大的收获，这一点我坚信。&lt;/p&gt;
&lt;p&gt;看了很多paper，也明白了很多的model，剩下的部分应该就是动手实践了。在选择框架的路上走过一些弯路，最开始用纯python写过一些简单的nnlm这样的模型，后来觉得用框架效率更高一些，于是尝试了keras，一个基于theano和tensorflow的框架，使用方法和torch差不多。如果只是解决一些常用的model结构的话，用keras非常地easy，代码量非常小，非常容易上手，比如rnn，cnn等等。但如果你想实现一个稍微复杂的model，对灵活性要求比较高的model，keras就有点捉襟见肘了，毕竟是一个框架上的框架，灵活性肯定好不了。后来就决定试一下theano，毕竟是deep learning发源地之一出的框架，github上开源了很多的程序都是用theano写的，而且自己也比较擅长python，于是就开始了theano的学习之路，其中最吸引我的是自动求导的功能，但最终导致我放弃theano的一个重要的原因是每次报错都让我特别头疼，因为根本没法找到错误的地方。挣扎了几天，通过重新调研，我选择了Torch，Torch是用lua封装的，意味着我得先学习lua，然后就开始torch，习惯了python的简单，用lua时感觉特别恶心，说不出原因的恶心，后来强忍着开始看一些demo，当初给自己的一个目标是用torch写出seq2seq+attention，然后做一些好玩的事情，于是找了HarvardNLP开源的代码&lt;a href=&quot;https://github.com/harvardnlp/seq2seq-attn&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;seq2seq-attn&lt;/a&gt;，这个组非常年轻，但非常nice，paper的code都会开源供大家学习。第一次看这个代码就觉得，写的好他么凌乱啊，根本没法读，现在来读的话也觉得很恶心，感觉写代码的人没有太多的规范，想到哪里写到哪里，于是就放弃了这个demo。后来跟着oxford的&lt;a href=&quot;https://github.com/oxford-cs-ml-2015/practical6&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;课程代码&lt;/a&gt;开始一步一步地学习torch，这个project是实现一个char-level语言模型，里面的功能虽然不够完善，但从最基础的东西开始写起，并没有用类似于rnn，dp这样的框架来做，非常适合入门学习和打基础。跟着这个project，也看着&lt;a href=&quot;https://github.com/karpathy/char-rnn&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;char-rnn&lt;/a&gt;，其实char-rnn也是跟着oxford的这个程序学着写的，很多的套路和源码都一样。经过了一段时间的挣扎，从0开始写起，实现了一个seq2seq+attention的torch源码，我个人认为比seq2seq-attn那个代码更加清晰和简洁，过段时间整理下会放在Github上。torch有很多的缺点，比如需要学习lua，处理文本不方便，demo不给力。所以，在文本处理这一块，我用了python+hdf5的方式来解决，因为用python处理文本太方便了，然后将处理好的结果放入hdf5中，让torch去调用。demo这块是一个很大的关，只要自己可以动手写出一个完整的project之后，后面的事情就好办了。最近一段时间，tensorflow在网上非常火，有很多的文章和微博大号都在热推他，我觉得框架各有各好，坚持用好一个就会很了不起了，不必盲目跟风。&lt;/p&gt;
&lt;p&gt;以上的部分是我从接触nlp开始，到现在的一个学习过程，下面的部分谈谈我的一些感触。&lt;/p&gt;
&lt;p&gt;deep learning从哪年火起来，并不重要，重要的是它还能火多久？会不会遭遇另一个寒冬？媒体的热捧是跟风、炒作还是客观？现在如果你不聊两句deep learning都不好意思出门。我认为对一个知识的理解大概有三个level：是什么？怎么样？为什么？&lt;/p&gt;
&lt;p&gt;第一个level，是什么的问题。RNN、CNN、RCNN、CRNN、FNN、DNN各种各样的NN充斥在各大媒体上，每天都有大量的文章来介绍各种NN做了什么牛逼的事情，哪些牛逼的机构提出了一种新的NN，将会给人类带来前所未有的方便等等等等。大家通过看一些博客，看一些新闻媒体都会了解到这些NN的概念和作用，以及类似于端到端、注意力模型的这样的概念。&lt;/p&gt;
&lt;p&gt;第二个level，是怎么样的问题。知道概念并不难，做出来才是真好汉！如何提出一个自己的模型，然后用熟悉的框架编程实现它，跑分排名发paper。这个level应该是比较难的level，可能也是一些学生处于的level。之前记得王威廉老师发过一个讨论帖：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;跟大家探讨几个开放式问题：大家认为到底计算机科学(Science)与工程(Engineering)的边界在哪里？以深度学习来说，学术界和工业界的着重点应该有什么不同与相同之处？如果本科毕业生也能玩转Theano/TensorFlow/Torch等平台的话，那么深度学习的博士优势到底何在？&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果大家可以玩转torch之类的框架，并且实现自己提出的模型，然后跑分刷榜发paper，那么做深度学习研究的博士优势又会在哪里呢？&lt;/p&gt;
&lt;p&gt;另外一种流传于网络的说法是，deep learning就是比谁更会调参数，这里不得不祭出这张图：&lt;/p&gt;
&lt;img src=&quot;http://ww2.sinaimg.cn/mw690/ba115fdfjw1f4nwmxwkn4j20ak05x74m.jpg&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;非常地讽刺，我想应该不是这么简单。&lt;/p&gt;
&lt;p&gt;还有一点是model这部分，可能是文章看的不够深，看到的paper都是从model这个层次来创新的，基本搞清楚整个数据流，输入和输出然后就会比较清楚了，而文章中一般也会用一张图把model的数据流讲解地非常清楚。在基本的model上添加gate，用hierarchical来做，套用seq2seq加attention，用copy mechanism等等方式来提出新model，获得更好的结果，亦或者是将人工feature添加到模型中。model是一个非常灵活的东西，记得本科时参加数学建模竞赛就是在做这样的一件事情，根据问题来提出自己模型，往往都是从已有模型中进行一些改进。&lt;/p&gt;
&lt;p&gt;第三个level，是为什么的问题。我个人认为Phd应该能够回答出自己所研究领域的各种各样的为什么，回答出不同level的人提出的为什么，而不仅仅是会用一个框架，提出几个model，然后刷几篇顶会就博士毕业了。更重要的是对问题的思考和理解，尤其是深度地理解。我觉得一个Phd应该具备的能力是提出一个问题，分析一个问题，解决一个问题的能力，而不仅仅是简单重复别人的东西，或者是简单改进下别人的model。调参数是一个基本工作，属于工程型的范畴，为了达到一个更好的效果，需要具备这个基本能力，但并不等同于说deep learning就是调参数，这种说法太过荒唐可笑。框架也只是一个工具，至于你用torch，用tensorflow或者是自己手写都只是一种工具而已，也是一个基本能力，但并不是说deep learning就是用框架。这种说法同样很可笑。一个升级版的model也可以发paper，但是否你的model真的改变了研究现状，带来了革命，而或者只是在原来model的基础上添加了一些小的想法，刷了一下排行榜，这里引用下不久前ACL主席Christopher D. Manning在文章中写过一句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;只是刷分并没有意义，研究的意义应该是对问题本身的认识。虽然deep learning看着热闹，会议非常多，隔一段时间就会出现一个会议论文集，但仔细想想有仅仅有几篇paper是在做更大更重要的事情，大量的paper还是处于model创新这个level。当然，毕业对论文的要求是一个很大的压力，所以这样的现象也并不奇怪。数量的增加必定会带来质量的下降，浮躁的气息必定会让大家都变得急功近利。&lt;/p&gt;
&lt;p&gt;以上是一个初学者对deep learning的一个非常浅薄的认识，很多观点只是这一阶段的观点，可能随着学习的深入会有不同的想法。看paper也有一段时间了，心中一直有些疑问，比如：&lt;/p&gt;
&lt;p&gt;1、gate函数的提出是基于怎样的一种情况，为什么要用gate而不是别的？lstm，gru等单元都采用了gate函数，通过这个函数成功地解决了rnn的长程依赖问题。&lt;/p&gt;
&lt;p&gt;2、miniBatch中batch的大小为什么会影响结果的优劣，包括其他的超参数，可不可以给出一些偏理论的分析，而不只是说大家都这么用，这就是经验这样的说法。&lt;/p&gt;
&lt;p&gt;3、hierarchical是不是都会比non-hierarchical更好呢？分层之后的什么导致了更好的结果？&lt;/p&gt;
&lt;p&gt;4、optimization是一个数学味道非常浓的学科，那么很多的model都采用sgd，或者是adam，或者是rmsprop，区别在哪里？哪种算法适合哪种model。&lt;/p&gt;
&lt;p&gt;5、deep learning到底多deep算deep，越deep越好？还是说到了一定的deep就ok了，神经网络是用来近似非线性函数的，那么可不可以计算出多深的网络可以以最低的误差来拟合函数？&lt;/p&gt;
&lt;p&gt;等等等等，心中有太多的疑惑，虽然现在可以提自己的model，也可以用框架来编程实现，但仍然回答不出上面的问题，那些为什么的问题。&lt;/p&gt;
&lt;p&gt;路漫漫其修远兮，吾将上下而求索。&lt;/p&gt;
&lt;p&gt;最后是广告时间，如果您对PaperWeekly做的事情感兴趣，可以关注下面的公众号，或者&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;订阅&lt;/a&gt;本博客。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;标题起的有一点装x了，昨天看到微博上刘知远老师对关于deep learning哪年火的问题的讨论，突然有一些自己的感触就写了下来。&lt;/p&gt;
&lt;p&gt;从接触nlp到现在大概过去了4个月的时间，最初的动机是要用word2vec工具包来给自己写的app(rsarxiv)添加一个pa
    
    </summary>
    
    
      <category term="deep learning" scheme="http://rsarxiv.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Gated-Attention Readers for Text Comprehension #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/</id>
    <published>2016-06-12T16:33:10.000Z</published>
    <updated>2016-06-12T17:35:02.000Z</updated>
    
    <content type="html">&lt;p&gt;完形填空一直是各大英语考试的常见题型，读一篇短文，填20个空。那么如果是机器来做完形填空，该如何来定义问题，提出模型呢？本周开始将会介绍一系列文本理解的模型。本文分享的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.01549v1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Gated-Attention Readers for Text Comprehension&lt;/a&gt;，最早于6月5日submit于arxiv上，作者是CMU的Graduate Research Assistant &lt;a href=&quot;https://www.cs.cmu.edu/directory/bdhingra&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Bhuwan Dhingra&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;首先，介绍一下对完形填空问题的定义。问题可以表述为一个三元组(d,q,a)，这里d是指原文document，q是指完形填空的问题query（这里需要注意一点的是，与我们英语考试中的完形填空不同，更像是只用一个单词来回答的阅读理解），a是问题的答案。这个答案是来自一个固定大小的词汇表A中的一个词。即：给定一个文档-问题对(d,q)，从A中找到最合适的答案a。&lt;/p&gt;
&lt;p&gt;本文精彩的部分有两个，一个是related work写的非常漂亮，另一个是提出了一种新的注意力模型GA（Gate-Attention） Reader，并且取得了领先的结果。&lt;/p&gt;
&lt;p&gt;后续的文本理解系列的文章将会从related work中产生，包括以下几篇：&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://arxiv.org/pdf/1506.03340v3.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Teaching machines to read and comprehend&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/pdf/1406.2710v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A multiplicative model for learning distributed text-based attribute representations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://www.cl.ecei.tohoku.ac.jp/publications/2016/kobayashi-dynamic-entity-naacl2016.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Dynamic entity representations with max-pooling improves machine reading&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&quot;http://arxiv.org/pdf/1603.01547v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Text understanding with the attention sum reader network&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&quot;http://arxiv.org/pdf/1511.02301v4.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;The goldilocks principle: Reading children’s books with explicit memory representations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] &lt;a href=&quot;http://arxiv.org/pdf/1606.02858v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面来介绍本文的模型，结合下图来看：&lt;/p&gt;
&lt;img src=&quot;/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;&lt;b&gt;step 1&lt;/b&gt; document和query通过一个Lookup层，使得每个词都表示成一个低维向量。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;step 2&lt;/b&gt; 将document中的词向量通过一个双向GRU，将两个方向的state做拼接获得该词的新表示。同时也将query通过一个双向GRU，用两个方向上的last hidden state作为query的表示。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;step 3&lt;/b&gt; 将document中每个词的新表示与query的新表示逐元素相乘得到下一个GRU层的输入。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;step 4&lt;/b&gt; 重复step 2和3，直到通过设定的K层，在第K层时，document的每个词向量与query向量做内积，得到一个最终的向量。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;step 5&lt;/b&gt; 将该向量输入到softmax层中，做概率归一化。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;step 6&lt;/b&gt; 因为document中有重复出现的词，聚合之后得到最终的分类结果，即确定应该填哪个词。&lt;/p&gt;
&lt;p&gt;模型的计算流程还是很好理解的，下面给出一些可视化的attention结果。&lt;/p&gt;
&lt;img src=&quot;/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig2.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;图中高亮的部分是针对问题时的最后一层注意力所关注的地方。&lt;/p&gt;
&lt;p&gt;注意力模型是一个非常热门的研究领域，很多专家都看好其在今后各大nlp任务中的应用前景，不同版本、不同结构、不同层次的注意力模型丰富了模型，也提升了效果。注意力的本质就是说你关注的输出与你的输入中的哪个元素关系更加紧密，即输出的部分应该更加注意哪个输入细节，在做完形填空、阅读理解的时候，我们也会有这样的感受，就是题目的答案往往就在某一句话或某几句话当中，并不需要回答每个问题都从全文中找一遍答案，而是定位到关键句上。这里的定位就是注意力，剩下的问题就是研究如何更加准确地定义、建模注意力，是用普通的前馈神经网络，还是用GRU，还是用分层模型都需要针对具体问题的特点。&lt;/p&gt;
&lt;p&gt;后续的几篇文章将会继续介绍文本理解，敬请关注。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;完形填空一直是各大英语考试的常见题型，读一篇短文，填20个空。那么如果是机器来做完形填空，该如何来定义问题，提出模型呢？本周开始将会介绍一系列文本理解的模型。本文分享的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.01549v1&quot; tar
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="Text Comprehension" scheme="http://rsarxiv.github.io/tags/Text-Comprehension/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly文章分类导航</title>
    <link href="http://rsarxiv.github.io/2016/06/11/PaperWeekly%E6%96%87%E7%AB%A0%E5%88%86%E7%B1%BB%E5%AF%BC%E8%88%AA/"/>
    <id>http://rsarxiv.github.io/2016/06/11/PaperWeekly文章分类导航/</id>
    <published>2016-06-11T23:48:38.000Z</published>
    <updated>2016-06-12T00:08:13.000Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Neural-Language-Model&quot;&gt;&lt;a href=&quot;#Neural-Language-Model&quot; class=&quot;headerlink&quot; title=&quot;Neural Language Model&quot;&gt;&lt;/a&gt;Neural Language Model&lt;/h1&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/&quot;&gt;A Neural Probabilistic Language Model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/&quot;&gt;Character-Aware Neural Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/&quot;&gt;Gated Word-Character Recurrent Language Model&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Word-Embeddings&quot;&gt;&lt;a href=&quot;#Word-Embeddings&quot; class=&quot;headerlink&quot; title=&quot;Word Embeddings&quot;&gt;&lt;/a&gt;Word Embeddings&lt;/h1&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/&quot;&gt;How to Generate a Good Word Embedding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/&quot;&gt;A Joint Model for Word Embedding and Word Morphology&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Sentence-Embeddings&quot;&gt;&lt;a href=&quot;#Sentence-Embeddings&quot; class=&quot;headerlink&quot; title=&quot;Sentence Embeddings&quot;&gt;&lt;/a&gt;Sentence Embeddings&lt;/h1&gt;&lt;h2 id=&quot;Comparison&quot;&gt;&lt;a href=&quot;#Comparison&quot; class=&quot;headerlink&quot; title=&quot;Comparison&quot;&gt;&lt;/a&gt;Comparison&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/&quot;&gt;Learning Distributed Representations of Sentences from Unlabelled Data&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Unsupervised-Learning&quot;&gt;&lt;a href=&quot;#Unsupervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Unsupervised Learning&quot;&gt;&lt;/a&gt;Unsupervised Learning&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/&quot;&gt;Distributed Representations of Sentences and Documents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/&quot;&gt;Skip-Thought Vectors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/&quot;&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Supervised-Learning&quot;&gt;&lt;a href=&quot;#Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Supervised Learning&quot;&gt;&lt;/a&gt;Supervised Learning&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/&quot;&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/&quot;&gt;Recurrent Convolutional Neural Networks for Text Classification&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Semi-Supervised-Learning&quot;&gt;&lt;a href=&quot;#Semi-Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Semi-Supervised Learning&quot;&gt;&lt;/a&gt;Semi-Supervised Learning&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/&quot;&gt;Semi-supervised Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Seq2Seq&quot;&gt;&lt;a href=&quot;#Seq2Seq&quot; class=&quot;headerlink&quot; title=&quot;Seq2Seq&quot;&gt;&lt;/a&gt;Seq2Seq&lt;/h1&gt;&lt;h2 id=&quot;Machine-Translation&quot;&gt;&lt;a href=&quot;#Machine-Translation&quot; class=&quot;headerlink&quot; title=&quot;Machine Translation&quot;&gt;&lt;/a&gt;Machine Translation&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstractive-Summarization&quot;&gt;&lt;a href=&quot;#Abstractive-Summarization&quot; class=&quot;headerlink&quot; title=&quot;Abstractive Summarization&quot;&gt;&lt;/a&gt;Abstractive Summarization&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/&quot;&gt;Neural Network-Based Abstract Generation for Opinions and Arguments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/18/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89/&quot;&gt;Incorporating Copying Mechanism in Sequence-to-Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/17/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/&quot;&gt;Neural Headline Generation with Minimum Risk Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/11/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B9%9D%EF%BC%89/&quot;&gt;LCSTS: A Large Scale Chinese Short Text Summarization Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/10/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AB%EF%BC%89/&quot;&gt;AttSum: Joint Learning of Focusing and Summarization with Neural Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] &lt;a href=&quot;http://rsarxiv.github.io/2016/05/07/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B8%83%EF%BC%89/&quot;&gt;Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] &lt;a href=&quot;http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/&quot;&gt;A Neural Attention Model for Abstractive Sentence Summarization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] &lt;a href=&quot;http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/&quot;&gt;Abstractive Sentence Summarization with Attentive Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] &lt;a href=&quot;http://rsarxiv.github.io/2016/04/24/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%BA%94%EF%BC%89/&quot;&gt;Generating News Headlines with Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Text-Entailment&quot;&gt;&lt;a href=&quot;#Text-Entailment&quot; class=&quot;headerlink&quot; title=&quot;Text Entailment&quot;&gt;&lt;/a&gt;Text Entailment&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/&quot;&gt;REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Dialogue-Generation&quot;&gt;&lt;a href=&quot;#Dialogue-Generation&quot; class=&quot;headerlink&quot; title=&quot;Dialogue Generation&quot;&gt;&lt;/a&gt;Dialogue Generation&lt;/h2&gt;&lt;p&gt;[1] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/&quot;&gt;Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://rsarxiv.github.io/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/&quot;&gt;A Neural Conversational Model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Neural-Language-Model&quot;&gt;&lt;a href=&quot;#Neural-Language-Model&quot; class=&quot;headerlink&quot; title=&quot;Neural Language Model&quot;&gt;&lt;/a&gt;Neural Language Model&lt;/
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>Neural Network-Based Abstract Generation for Opinions and Arguments #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/</id>
    <published>2016-06-10T19:34:27.000Z</published>
    <updated>2016-06-10T22:03:19.000Z</updated>
    
    <content type="html">&lt;p&gt;本篇将会分享的是一篇工程性比较强的paper，如果您想做一个实实在在的意见摘要系统（比如：淘宝商品评论摘要、电影评论摘要）的话，可以仔细研读下本文的解决方案。本文的题目是&lt;a href=&quot;http://arxiv.org/pdf/1606.02785v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Neural Network-Based Abstract Generation for Opinions and Arguments&lt;/a&gt;，于6月9日submit于arxiv上。作者是来自美国东北大学的&lt;a href=&quot;http://www.ccs.neu.edu/home/luwang/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lu Wang&lt;/a&gt;助教。&lt;/p&gt;
&lt;p&gt;关于&lt;a href=&quot;http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/&quot;&gt;自动文摘&lt;/a&gt;，之前写过一系列的文章，包含了自动文摘的方方面面以及近期的一些相关paper的详细描述。本文的自动文摘问题是一个多评论摘要问题，用的是abstractive方法，而非简单的extractive方法，就是说从多个评论中总结出观点。&lt;/p&gt;
&lt;p&gt;本文模型的主题框架仍是seq2seq+attention，最主要的不同之处是输入包括多个文本序列，而是之前介绍的单文本序列。这里，seq2seq+attention的思路不再赘述，主要讲一下不同的地方。&lt;/p&gt;
&lt;p&gt;为了套用seq2seq，本文将多文本拼接成单文本，中间用特殊的标记SEG隔开。但是如果只是简单的套用seq2seq的话，会存在以下两个问题：&lt;/p&gt;
&lt;p&gt;1、seq2seq对序列的顺序非常敏感，多个文本排列的顺序对结果的影响比较大。&lt;/p&gt;
&lt;p&gt;2、多篇评论包括的词会比较多，会导致在计算attention的时候花费更大的时间代价。&lt;/p&gt;
&lt;p&gt;本文用了子采样(sub-sampling)的方法来解决上面的问题，首先给原始输入中的每个评论定义importance score，然后归一化，最后从原始输入中进行多项分布采样，获得K个候选sample作为seq2seq的输入数据，进行训练。本文针对importance score建立了一个回归模型，使用了一些人工feature作为输入进行回归打分。这些feature如下表所示：&lt;/p&gt;
&lt;img src=&quot;/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig1.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;包括了词的数量，命名实体的数量，tf-idf平均数和最大数等8个feature作为输入。通过学习这个回归模型，来计算给定评论的分数。&lt;/p&gt;
&lt;p&gt;最后给大家展示一个结果图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;本文在模型上创新的点并不突出，最不同以往的地方便是用了人工feature来给每个评论打分，给原始输入中的评论进行排序，然后多项分布采样，子采样的过程是一个降维的过程，保留了原始数据中最重要的部分，去掉了冗余的信息。可以说本文是将人工features添加到abstractive来提升纯粹的seq2seq模型性能，针对了多文档摘要问题的特点，给出了一个实用性较强的思路。如果从模型角度来说，新的东西没有太多，而且可改进的地方有很多，比如打分模型，可以用sentence representation的思路来做，完全可以避免用人工feature这种比较low的思路，做成一个data-driven的打分模型；再比如，不用打分，而是采用CNN从多个评论中提取出最有用的feature作为输入。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本篇将会分享的是一篇工程性比较强的paper，如果您想做一个实实在在的意见摘要系统（比如：淘宝商品评论摘要、电影评论摘要）的话，可以仔细研读下本文的解决方案。本文的题目是&lt;a href=&quot;http://arxiv.org/pdf/1606.02785v1.pdf&quot; targ
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
      <category term="自动文摘" scheme="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/"/>
    
  </entry>
  
  <entry>
    <title>A Joint Model for Word Embedding and Word Morphology #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/</id>
    <published>2016-06-09T21:34:03.000Z</published>
    <updated>2016-06-10T05:27:33.000Z</updated>
    
    <content type="html">&lt;p&gt;大家端午节快乐！本文将分享一篇关于词向量模型最新研究的文章，文章于6月8号提交到arxiv上，题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.02601v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Joint Model for Word Embedding and Word Morphology&lt;/a&gt;，作者是来自剑桥大学的博士生&lt;a href=&quot;https://www.cl.cam.ac.uk/~kc391/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Kris Cao&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;本文最大的贡献在于第一次将词形联合词向量一同进行训练，从某种程度上解决了未登录词（OOV）的词向量表示问题，同时也得到了一个效果不错的词形分析器。&lt;/p&gt;
&lt;p&gt;介绍本文模型之前先简单介绍下本文中采用的词向量训练方法，skip-gram with negative sampling（SGNS）。这个方法是word2vec中的一种方法，大概的思路是可参见下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig3.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;通过用dog这个词来预测其上下文，比如cute、fluffy、barked、loudly，为了更快地收敛，增加负样本，即图中的bicycle和Episcopal这两个与dog无关的词。skip-gram的思路就是通过word来预测上下文context，而negative sampling则是根据当前词构造出一些与之无关的词，作为负样本加速收敛。&lt;/p&gt;
&lt;p&gt;接下来介绍本文的模型Char2Vec，将字符作为最小的单元进行研究，因为对于字符这个层次来说，并不会出现OOV词的情况。具体看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;在每个单词的首和尾分别添加符号^和$作为标记，将词看作是一个字符序列。在这个序列上用一个正向LSTM和一个反向LSTM得到两组hidden state，每个位置上的字符都对应着两个hidden state，将其拼接起来，然后用一个单层前馈神经网络进行处理，得到该位置上的hidden state，记为h(i)。有了每个字符的表示，接下来用attention机制来构造出词的表示，即学习一个权重系数，来表明这个词的语义与哪个h(i)关系更大，一般来说词干所在的h(i)权重会大一些，词前缀或者后缀并不能表示语义，所以权重会小很多。见下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;图中的单词malice、hatred、greed会与序列中的词干spit、spite，前缀^、后缀ful关系就会紧密一些，而与其他错误的字符串关系不大。&lt;/p&gt;
&lt;p&gt;通过attention model我们得到了词向量f(w)。剩下的过程就是用skip-gram with negative sampling来训练词向量了。先前的工作都是用lstm处理字符序列来表示整个单词向量，本文并没有这样做，而是将直接使用attention model来获取每个h(i)中的信息，包括一个正向的lstm和反向的lstm，正向的lstm包含了词干和词前缀，反向的lstm包含了词后缀。当我们处理未知的词时，可以将这个词分解为已知的部分和未知的部分，这个模型就可以通过已知的部分来预测整个词的词向量，因此解决了OOV的问题。&lt;/p&gt;
&lt;p&gt;实验中测试了该模型的词形分析的能力，尤其是在单词词形很丰富（包括词干、前缀、后缀）的情况下，效果优于一些成熟的分词器。看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig4.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;在词向量效果测试中，本文模型在semantic测试中表现很差，但在syntactic测试中表现非常好。看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig5.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;看得出来本文模型的优势非常明显，优势在于解决了大量处于长尾尾端的合成词的词向量表示问题，通过用未知词的已知部分（词干）来预测该词的词向量，从而解决了word2vec等一系列前人工作中未解决的问题，在英语语境中效果可能没那么好，如果换作是德语或者土耳其语这种词形非常丰富的语言会有更好的效果。在整个任务评测中，可以更好地解决syntactic相似问题，因为引入了词形这个feature可以很好地解决syntactic任务；而在semantic任务中却表现非常差，原因是char-level的词向量模型在捕捉语义上效果本身就不如word-level的模型。可以说，本文在传统词向量模型的基础上考虑加入feature来提升性能，是一种非常积极的尝试，虽然并没有在方方面面上都得到改善，但毕竟是一个探索性的、且非常有意义的工作，值得学习。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;大家端午节快乐！本文将分享一篇关于词向量模型最新研究的文章，文章于6月8号提交到arxiv上，题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.02601v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Joi
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="word embedding" scheme="http://rsarxiv.github.io/tags/word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Gated Word-Character Recurrent Language Model #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/</id>
    <published>2016-06-08T16:00:50.000Z</published>
    <updated>2016-06-08T16:31:44.000Z</updated>
    
    <content type="html">&lt;p&gt;本篇将会分享一篇最新的paper，2016年6月6日submit到arxiv上，paper的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.01700v1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Gated Word-Character Recurrent Language Model&lt;/a&gt;，作者是来自纽约大学的硕士生&lt;a href=&quot;http://cds.nyu.edu/people/yasumasa-miyamoto/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Yasumasa Miyamoto&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;语言模型或者说一切自然语言生成的问题都面临着一个严峻的挑战就是未登录词（OOV），一般的语言模型处理方法都是将前N个高频词当做词表，后面的低频词都用unk来代替，而且所有的低频词都用同一个词向量来表示。本文的最大贡献在于提出了一种混合char-level和word-level的语言模型，通过一种gate机制来选择是用char-level来表示一个词向量，还是直接用word-level来表示一个词向量。char-level模型的优势在于解决低频词的表达，很多之前分享过的模型都是用char来作为基本单元。&lt;/p&gt;
&lt;p&gt;本文的模型并不复杂，思路也非常清晰，如下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;模型分为两个部分：&lt;/p&gt;
&lt;p&gt;1、词向量。模型中的词向量由两部分综合而成。第一部分是传统的词向量，每一个词都用一个低维实向量来表示，第二部分是将每个词认为是一个char-level的序列，用一个双向LSTM来表示这个词。两部分词向量由一个门函数来决定使用哪个，如下式：&lt;/p&gt;
&lt;img src=&quot;/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig2.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;门函数我们见过太多了，尤其是在LSTM和GRU中，各种各样的门函数来控制信息的流动，本文模型中采用了一种非常简单的机制来决定采用哪种词向量，高频词的话，一定是采用传统的word-level方式，直接从lookup table中读取；低频词的话，用char-level的方式获得一个更好的表示。这里需要注意的一点是，门函数的值，也就是说每个单词用哪种词向量是与上下文无关的，只要是同一个单词，就会采用相同的选择方式。&lt;/p&gt;
&lt;p&gt;2、语言模型。这个部分就非常简单了，就是一个典型的RNNLM，这里的隐藏单元采用LSTM。&lt;/p&gt;
&lt;p&gt;实验部分选了三个baseline，（1）仅仅用word-level，（2）仅仅用char-level，（3）将两种词向量拼接。在三个数据集上进行了测试，本文模型比起baseline具有明显的优势。&lt;/p&gt;
&lt;p&gt;最后简单讨论了门函数值与词出现的频率之间的关系，如下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig3.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;本文采用了一中混合模型，然后用gate mechanism从多个模型中进行选择。这种思路有一种似曾相识的感觉，好比是参加kaggle比赛，通常一个分类器并不能得到最好的结果，混合使用多个分类器往往会得到更好的结果。本文的感觉有一点类似，用了char-level的优势来弥补word-level的劣势，从而取得更好的效果。也是一种很好的启发。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本篇将会分享一篇最新的paper，2016年6月6日submit到arxiv上，paper的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.01700v1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Gated Word
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="RNNLM" scheme="http://rsarxiv.github.io/tags/RNNLM/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Semi-supervised Sequence Learning #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/</id>
    <published>2016-06-07T14:11:34.000Z</published>
    <updated>2016-06-07T15:27:42.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;之前分享过几篇有监督的sentence表示方法，比如&lt;a href=&quot;http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/&quot;&gt;Recurrent Convolutional Neural Networks for Text Classification&lt;/a&gt;、&lt;a href=&quot;http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/&quot;&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt;，也分享过很多几篇无监督的sentence表示方法，比如&lt;a href=&quot;http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/&quot;&gt;Distributed Representations of Sentences and Documents&lt;/a&gt;、&lt;a href=&quot;http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/&quot;&gt;Skip-Thought Vectors&lt;/a&gt;。本篇将分享是一篇半监督的sentence表示方法，该方法比Paragraph Vectors更容易做微调，与Skip-Thought相比，目标函数并没有它那么困难，因为Skip-Thought是用来预测相邻句子的。本文的题目是&lt;a href=&quot;http://arxiv.org/pdf/1511.01432.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Semi-supervised Sequence Learning&lt;/a&gt;，作者是来自Google的&lt;a href=&quot;http://homepages.inf.ed.ac.uk/s0681274/About_Me.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Andrew M. Dai&lt;/a&gt;博士。&lt;/p&gt;
&lt;p&gt;纯粹的有监督学习是通过神经网络来表示一个句子，然后通过分类任务数据集去学习网络参数；而纯粹的无监督学习是通过上文预测下文来学习句子表示，利用得到的表示进行分类任务。本文的方法将无监督学习之后的表示作为有监督训练模型的初始值，所以称为半监督。本文的有监督模型采用LSTM，无监督模型共两种，一种是自编码器，一种是循环神经网络语言模型。&lt;/p&gt;
&lt;p&gt;第一种模型称为Sequence AutoEncoder LSTM(SA-LSTM)，模型架构图如下：&lt;/p&gt;
&lt;img src=&quot;/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/fig1.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;这幅图大家看着都眼熟，和&lt;a href=&quot;http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;中的seq2seq架构图很相似，只不过target和input一样，即用input来预测input自己。将自编码器学习到的表示作为LSTM的初始值，进行有监督训练。一般来说用LSTM中的最后一个hidden state作为输出，但本文也尝试用了每个hidden state权重递增的线性组合作为输出。这两种思路都是将无监督和有监督分开训练，本文也提供了一种联合训练的思路作为对比，称为joint learning。&lt;/p&gt;
&lt;p&gt;第二种模型称为Language Model LSTM(LM-LSTM)，将上图中的encoder部分去掉就是LM模型。语言模型介绍过很多了，比如&lt;a href=&quot;http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/&quot;&gt;A Neural Probabilistic Language Model&lt;/a&gt;和&lt;a href=&quot;http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/&quot;&gt;Character-Aware Neural Language Models&lt;/a&gt;，详细的可以看之前的分享，这里不再赘述了。&lt;/p&gt;
&lt;p&gt;模型部分就是这些，后面作者在情感分析、文本分类、目标分类等多组任务中进行了对比实验，均取得了不错的结果。&lt;/p&gt;
&lt;p&gt;本文的创新点在于结合了无监督和有监督学习两种思路的优点来解决一个传统问题，虽然说无监督是一种趋势所在，但有监督针对具体的问题会有更好的效果。这种融合各类模型优点的模型会更受欢迎，也是一种不错的思路。&lt;/p&gt;
&lt;p&gt;这里进行几点说明：&lt;/p&gt;
&lt;p&gt;1、为什么不对实验结果进行详细的介绍？&lt;/p&gt;
&lt;p&gt;因为我个人更加关注的是解决问题的思路，也就是模型部分；另一方面，paper中的实验结果只能在某些程度上说明问题，对比结果中的数据可能是作者精心挑选的最好数据，并不一定可以复现，所以我不会太纠结于到底哪个模型比哪个模型高几个百分点。而文中的模型思路会带给我更多的启发，所以更加有意义一些。&lt;/p&gt;
&lt;p&gt;2、为什么内容总是这么短？&lt;/p&gt;
&lt;p&gt;因为我对PaperWeekly的定位是每天一篇或者几天一篇的paper短文介绍和理解，并不是详细地剖析它，我希望内容尽可能短，大家可以用5-10分钟来明白一篇文章的贡献和创新点在哪里，更多的是为了带给大家更多的思考或者说是启发。另外一个方面，短的文章我写起来也会很快，基本上都是前一晚睡觉前来读，六点半早起来写，不影响一天的正常工作生活，却一天一天地在积累着。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;之前分享过几篇有监督的sentence表示方法，比如&lt;a href=&quot;http://rsarxiv.github.io/2
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>A Hierarchical Neural Autoencoder for Paragraphs and Documents #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/</id>
    <published>2016-06-06T14:09:52.000Z</published>
    <updated>2016-06-07T15:22:33.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;本篇将会分享一篇用自动编码器(AutoEncoder)来做文档表示的文章，本文的结果会给自然语言生成、自动文摘等任务提供更多的帮助。本文作者是来自斯坦福大学的博士生&lt;a href=&quot;http://web.stanford.edu/~jiweil/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Jiwei Li&lt;/a&gt;，简单看了下其简历，本科居然是北大生物系的，是一个跨界选手。本文的题目是&lt;a href=&quot;http://arxiv.org/pdf/1506.01057.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;，于2015年6月放在arxiv上。&lt;/p&gt;
&lt;p&gt;自动编码器之前接触的并不多，所以读了下Yoshua Bengio的deep learning一书补了一下知识。其实挺简单的，就是通过构造一个网络来学习x-&amp;gt;x，最简单的原型就是h=f(x)，x=g(h)。如果输入和输出的x都是完全一样的话，那么就没什么意义了。一般来说，后一个x会与前一个x有一些“误差”或者说“噪声”。而且自动编码器关注的是中间层h，即对输入的表示。如果h的维度小于x的维度，学习这个表示其实就是一个降维的过程。自动编码器有很多种类型，这里就不一一赘述了。&lt;/p&gt;
&lt;p&gt;本文的贡献在于用分层LSTM模型来做自动编码器。模型分为三个，为递进关系。&lt;/p&gt;
&lt;p&gt;1、标准的LSTM，没有分层。模型结构看起来和最简单的seq2seq没有区别，只是说这里输入和输出一样。看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;2、分层LSTM。这里分层的思想是用句子中的所有单词意思来表示这个句子，用文档中的所有句子意思来表示这个文档，一层接一层。看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;在word这一层，用一个标准的LSTM作为encoder，每一句中的最后一个word的hidden state作为该句的state，在sentence这一层，文档中所有的句子构成一个序列，用一个标准的LSTM作为encoder，得到整个文档的表示。decoder部分同样是一个分层结构，初始state就是刚刚生成的文档表示向量，然后先decoder出sentence这一层的表示，然后再进入该sentence对其内部的word进行decoder。&lt;/p&gt;
&lt;p&gt;3、分层LSTM+Attention，这里的Attention机制和之前分享的是一样的，并且只在sentence这一层用了attention，参看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig3.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;在decoder部分中生成句子表示时，会重点注意输入中与该句子相关的句子，也就是输入中与之相同的句子。这里注意力的权重与&lt;a href=&quot;http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate &lt;/a&gt;中的计算方法一样。&lt;/p&gt;
&lt;p&gt;在实验中验证了本文模型的有效性，并且经过对比验证了第三种模型的效果最好，其次是第二种，最差的第一种，也与预期的相符。&lt;/p&gt;
&lt;p&gt;昨天分享的也是一个分层模型，相比于单层的模型效果更好一些，这是否可以引起一些思考？本文也提到后面可以将本文的这种思想应用到自动文摘、对话系统、问答系统上。虽然seq2seq+attention已经在这几大领域中取得了不错的成绩，但如果改成分层模型呢，是不是可以取得更好的成绩？是不是可以将本文的input和output换作自动文摘中的input和target，然后用同样的方法来解决呢？我想应该是可以的。&lt;/p&gt;
&lt;p&gt;另外，因为我个人比较关注自动文摘技术，自动文摘中abstractive类的方法一般都会涉及到Paraphrase（转述，换句话说），本文的自动编码器模型正好很适合做Paraphrase，输入一句话或者一段话，得到一个带有“误差”的语句通顺的版本。一种最简单的思路，用传统的方法提取出文中最重要的几句话（extractive式的方法），用Paraphrase处理一下得到文本摘要。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;本篇将会分享一篇用自动编码器(AutoEncoder)来做文档表示的文章，本文的结果会给自然语言生成、自动文摘等任务提供更
    
    </summary>
    
    
      <category term="Autoencoder" scheme="http://rsarxiv.github.io/tags/Autoencoder/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/</id>
    <published>2016-06-05T17:57:48.000Z</published>
    <updated>2016-06-07T15:23:20.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;昨天介绍了一篇工程性比较强的paper，关于对话生成（bot）任务的，今天继续分享一篇bot方面的paper，6月2日刚刚submit在arxiv上。昨天的文章用了一种最最简单的端到端模型来生成对话，取得了不错的结果，而本文用了一种更加复杂的模型来解决这个问题，取得了更好的结果。文章的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1606.00776v1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation&lt;/a&gt;，作者是来自蒙特利尔大学的博士生&lt;a href=&quot;https://mila.umontreal.ca/en/person/iulian-vlad-serban/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Iulian Vlad Serban&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;本文最大的贡献在于提出了一种多尺度循环神经网络（Multiresolution RNN,MrRNN），这里的多尺度是指描述文本序列的方式有多种尺度，不仅仅是传统的用一个又一个word来表示序列，这种表示称为自然语言表示，还包括了一种所谓的high-level信息来表示文本序列，这种表示称为粗糙序列表示。本文的模型受启发于分层循环端到端模型（Hierarchical Recurrent Encoder-Decoder，HERD），该模型应用于搜索领域，将用户的search session划分为两个层次的序列，一个是query的序列，一个是每个query中词的序列。&lt;/p&gt;
&lt;p&gt;本文模型中一个非常重要的部分是数据的预处理，将训练数据中的所谓high-level信息提取出来构造第二种序列来表示整个文本，这里用了两种思路。&lt;/p&gt;
&lt;p&gt;1、提取文本中的名词。用词性标注工具提取出文本中的名词，去掉停用词和重复的词，并且保持原始的词序，还添加了句子的时态。通过这个过程构造了一种表示原始文本的序列。&lt;/p&gt;
&lt;p&gt;2、提取文本中的动词和命名实体。用词性标注工具提取文本中的动词，并标记为activity，然后通过一些其他工具从所有训练数据中构造了一个命名实体的词典，帮助提取原句中的命名实体。因为数据集是ubuntu对话数据集，会涉及到大量的linux命令，所以还构造了一个linux命令词典，以标记原句中的命令。同样地也添加了句子的时态。通过这个处理过程，构造了另外一种表示原始文本的序列。&lt;/p&gt;
&lt;p&gt;两种处理方法将原句用一种关键词的形式表示出来，尤其是第二种方法针对Ubuntu数据集的特点，包含了非常多的特征进来。这样的表示本文称为coarse sequence representation，包含了high-level的信息，比起单纯的word by word sequence具有更加丰富的意义。&lt;/p&gt;
&lt;p&gt;接下来，看一下本文模型的架构图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;模型中包括了两个层次，或者说是两种尺度，一种用了很多的词来表示一个句子，另外一种用了经过处理的包含了更加重要的信息的词来表示一个句子。下层生成的预测值将会作为上层docoder在做预测时的context的一部分，这部分context包含了重要的、high-level的信息，再加上上层自己encoder的输出也作为context，可以说这个模型的context包含了非常丰富的内容。理解上面的图，只要仔细看好箭头的指向，也就明白了各个部分的输入输出是哪些。每个time step的数据流过程如下：&lt;/p&gt;
&lt;p&gt;下层：coarse encoder -&amp;gt; coarse context -&amp;gt; coarse decdoer -&amp;gt; coarse predciton encoder&lt;/p&gt;
&lt;p&gt;上层：natural language encoder -&amp;gt; &lt;b&gt;(natural language context + coarse prediction encoder)&lt;/b&gt; -&amp;gt; natural language decoder -&amp;gt; natural language prediction&lt;/p&gt;
&lt;p&gt;不管是用自动评价指标还是人工评价，结果都表明了本文的模型效果比baseline要高出很多个百分点，远远好于其他模型。下面展示一个结果，是ubuntu数据集上的测试效果：&lt;/p&gt;
&lt;img src=&quot;/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;可以看的出本文模型生成的结果效果比其他模型好很多。&lt;/p&gt;
&lt;p&gt;本文模型并不是一个纯粹的数据驱动的模型，在初始的阶段需要做一些非常重要的数据预处理，正是这个预处理得到的序列表示给本文的好结果带来了保证。我想，这种处理问题的思路可以推广到解决其他问题上，虽然本文模型很难直接应用到其他问题上，但我相信经过一些不大的变化之后，可以很好地解决其他问题，比如我一直关注的自动文摘问题，还有机器翻译、自动问答等等各种涉及到自然语言生成问题的任务上。这篇文章的结果也支持了我之前的一个观点，就是在解决问题上不可能存在银弹，不同的问题虽然可以经过一些假设变成相同的数学问题，但真正在应用中，不同的问题就是具有不同的特点，如果只是想用一种简单粗暴的data driven模型来解决问题的话，相信效果会不如结合着一些该问题feature的模型。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;昨天介绍了一篇工程性比较强的paper，关于对话生成（bot）任务的，今天继续分享一篇bot方面的paper，6月2日刚刚
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="bot" scheme="http://rsarxiv.github.io/tags/bot/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>A Neural Conversational Model #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/</id>
    <published>2016-06-04T17:57:31.000Z</published>
    <updated>2016-06-07T15:23:17.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面介绍过几篇seq2seq在机器翻译、文本蕴藏、自动文摘领域的应用，模型上每篇稍有不同，但基本的思想是接近的。本文继续分享一篇seq2seq在对话生成任务上的应用，是一篇工业界的论文，因此并没有什么理论创新。之所以选这一篇，是因为对话生成是一个非常热门的研究领域和应用领域，也可能是一个非常热门的创业领域，另外一个原因是为了充实seq2seq在各个领域中的应用这一主题。论文题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1506.05869.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Neural Conversational Model&lt;/a&gt;，作者是来自Google Brain，毕业于UC Berkeley的&lt;a href=&quot;http://www1.icsi.berkeley.edu/~vinyals/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Oriol Vinyals&lt;/a&gt;博士，论文最早于2015年7月放在arxiv上。&lt;/p&gt;
&lt;p&gt;模型部分不用多说，是最简单的seq2seq，架构图如下：&lt;/p&gt;
&lt;img src=&quot;/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig1.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;本篇主要想分享的东西是结果以及一些思考。文中采用了两个数据集，IT Helpdesk Troubleshooting dataset和OpenSubtitles dataset，前者是一个关于IT类的FAQ数据集，后者是一个电影剧本的数据集。&lt;/p&gt;
&lt;p&gt;我们可以看一下训练后的模型生成的对话结果，这里只关注第二个数据集的结果：&lt;/p&gt;
&lt;p&gt;常识类问题：&lt;/p&gt;
&lt;img src=&quot;/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig2.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;哲学类问题：&lt;/p&gt;
&lt;img src=&quot;/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig3.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;道德类问题：&lt;/p&gt;
&lt;img src=&quot;/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig4.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;为了对比，作者添加了一组&lt;a href=&quot;www.cleverbot.com&quot;&gt;cleverbot&lt;/a&gt;(cleverbot是一个在线聊天机器人)的对比结果，如下：&lt;/p&gt;
&lt;img src=&quot;/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig5.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;从对比结果中可以看得出，本文模型生成的结果比网上流行的在线聊天机器人要看起来更加“智能”一些，之前在知乎上回答过一个问题 &lt;a href=&quot;https://www.zhihu.com/question/46558198/answer/102722213?group_id=720215813641998336&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;三代聊天机器人在技术上的区别在哪里？&lt;/a&gt;，我想cleverbot更接近于第二代，采用了对话检索，即对话是从一个很庞大的数据库中匹配检索来的，而本文的模型属于第三代，更加智能，给定输入生成输出，并不需要借助于人工特征。&lt;/p&gt;
&lt;p&gt;但bot这个领域确实还面临一些问题，就像文中作者所说，如何客观地评价生成的效果非常重要，尤其是对于一些没有标准答案的问题来说，根本无法衡量哪个结果更加好。其实不仅仅bot，在自动文摘、机器翻译等各种nlp任务中，评价都是一个很难的问题，自动评价只是从某种意义上解决了各个模型相互比较的一种需求，但在实际应用当中用户的评价更加重要，虽然有时并不是那么客观。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面介绍过几篇seq2seq在机器翻译、文本蕴藏、自动文摘领域的应用，模型上每篇稍有不同，但基本的思想是接近的。本文继续分
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="bot" scheme="http://rsarxiv.github.io/tags/bot/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/</id>
    <published>2016-06-03T14:06:55.000Z</published>
    <updated>2016-06-07T15:22:26.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面几篇文章分享的都是seq2seq和attention model在机器翻译领域中的应用，在自动文摘系列文章中也分享了六七篇在&lt;a href=&quot;http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/&quot;&gt;自动文摘&lt;/a&gt;领域中的应用。本文将分享的这篇文章研究了seq2seq+attention在&lt;a href=&quot;https://en.wikipedia.org/wiki/Textual_entailment&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;textual entailment&lt;/a&gt;领域的应用。本文题目是&lt;a href=&quot;http://arxiv.org/pdf/1509.06664.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION&lt;/a&gt;，作者是来自英国伦敦大学学院的&lt;a href=&quot;http://rockt.github.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tim Rocktaschel&lt;/a&gt;博士（后面两个作者来自Google Deepmind），文章于2015年9月放在arxiv上，被ICLR 2016录用。&lt;/p&gt;
&lt;p&gt;首先，介绍一下文本蕴藏（textual entailment）是一个怎样的任务，简单点说就是用来判断两个文本序列之间的是否存在推断关系，是一个分类问题（具体可参见Wikipedia）。两个文本序列分别称为premise和hypothesis。&lt;/p&gt;
&lt;p&gt;本文最大的贡献在于：&lt;/p&gt;
&lt;p&gt;1、将end2end的思想应用到了文本蕴藏领域，取得了不错的效果。&lt;/p&gt;
&lt;p&gt;2、提出了一种seq2seq模型、两种attention模型和一种trick模型。&lt;/p&gt;
&lt;p&gt;本篇关注的重点在于四种模型的构建，来看模型架构图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;图中A是我们最熟悉的简单seq2seq模型，本文称为conditional encoding模型；B是本文提出的Attention模型，context与之前分享的都不一样；C是我们之前介绍过的attention模型，本文称为word-by-word attention模型。&lt;/p&gt;
&lt;p&gt;1、首先介绍A模型。将premise和hypothesis认为是source和target，即用encoder来处理premise，用decoder来处理hypothesis，只不过这里的decoder并不是一个语言模型，而只是一个和encoder一样的LSTM，decoder的输入是encoder的最后一个hidden state，对应图中的c5和h5。最后decoder的输出是一个联合表示premise和hypothesis的向量，用于最终的分类。&lt;/p&gt;
&lt;p&gt;2、介绍B模型。该任务和机器翻译不同，并不一定需要做所谓的soft alignment，而是只需要表示好两个句子之间的关系即可，因此这个模型的想法是将hypothesis的句子表示与premise建立注意力机制，而不是将hypothesis的每个单词都与premise做alignment。从上图中标记B的地方也可以看出，attention仅仅依赖于hypothesis的last hidden state。结果可以参看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;从图中可以看出hypothesis与premise中哪些词相关性更强。&lt;/p&gt;
&lt;p&gt;3、介绍C模型。这个模型与我们之前一直分享的attention模型一致，模型对hypothesis和premise每个单词做了alignment，所以这里称为word-by-word attention，从模型架构图中也可以看出，hypothesis中的每个词都与premise中对应的词进行了alignment。这里并不是生成单词，而是建立起两个文本序列之间的关系。结果可以参看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig3.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;图中表示的是alignment矩阵，更暗的地方表示这两个词更加相关。&lt;/p&gt;
&lt;p&gt;4、最后一种模型称为two-way模型，其实是一个trick，借鉴了BiRNN的思想，使用两个相同参数的LSTM，第一个LSTM从一个方向上对基于hypothesis的premise进行表示，而第二个LSTM从相反的方向上对基于premise的hypothesis进行表示，最终得到两个句子表示，拼接起来作为分类的输入。（过程与BiRNN类似，从两个方向上对hypothesis-premise进行了表示，可与前面的模型组合使用，从结果上来看并没有什么明显的作用）&lt;/p&gt;
&lt;p&gt;最后的实验结果表明，采用模型C，即word-by-word attention模型效果最好，其次是B模型，最差的是A模型。结果与预期基本符合，但加了two-way的效果并没有更好，反而更差。作者分析说用了相同的参数来做two-way可能会给训练给来更多的噪声影响，所以效果并不好。&lt;/p&gt;
&lt;p&gt;整体上来说，seq2seq+attention的组合给很多研究领域带来了春天，给了研究者们更多的启发，attention的形式有多种，可能针对不同的问题，不同的attention会带来不同的效果，也不好说哪一种一定更加适合某一个特定的任务，所以需要去不断地探索。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面几篇文章分享的都是seq2seq和attention model在机器翻译领域中的应用，在自动文摘系列文章中也分享了六
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="attention" scheme="http://rsarxiv.github.io/tags/attention/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Neural Machine Translation by Jointly Learning to Align and Translate #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/</id>
    <published>2016-06-02T19:30:30.000Z</published>
    <updated>2016-06-07T15:23:15.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面的两篇文章简单介绍了seq2seq在机器翻译领域的尝试，效果令人满意。上一篇也介绍到这一类问题可以归纳为求解P(output|context)的问题，不同的地方在于context的构建思路不同，上两篇中的seq2seq将context定义为encoder的last hidden state，即认为rnn将整个input部分的信息都保存在了last hidden state中。而事实上，rnn是一个有偏的模型，越靠后的单词在last state中占据的“比例”越高，所以这样的context并不是一个非常好的办法，本文将分享的文章来解决这个问题。题目是&lt;a href=&quot;http://arxiv.org/pdf/1409.0473.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;，作者是来自德国雅各布大学的&lt;a href=&quot;http://minds.jacobs-university.de/dima&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Dzmitry Bahdanau&lt;/a&gt;，现在是Yoshua Bengio组的一个博士生，文章于2015年4月放在arxiv上。&lt;/p&gt;
&lt;p&gt;本篇不再讨论seq2seq，如果您想了解seq2seq，可以去看&lt;a href=&quot;http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;和&lt;a href=&quot;http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation&lt;/a&gt;两篇博客。本篇只讨论不同的地方。&lt;/p&gt;
&lt;p&gt;本文用encoder所有hidden state的加权平均来表示context，权重表示decoder中各state与encoder各state的相关性，简单的seq2seq认为decoder中每一个state都与input的全部信息（用last state表示）有关，而本文则认为只与相关的state有关系，即在decoder部分中，模型只将注意力放在了相关的部分，对其他部分注意很少，这一点与人类的行为很像，当人看到一段话或者一幅图的时候，往往会将注意力放在一个很小的局部，而不是全部。具体看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig1.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;decoder中预测每个输出的条件概率变为：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig2.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;这里每个time step的state变为：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig3.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;这里，context vector由下式计算：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig4.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;权重用了一个最简单的mlp来计算，&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig6.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;然后做归一化：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig5.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;这里的权重反应了decoder中的state s(i-1)和encoder中的state h(j)之间的相关性。本文在为了得到相对来说无偏的state，在encoder部分采用了BiRNN。&lt;/p&gt;
&lt;p&gt;在机器翻译领域中，attention model可以理解为source和target words的soft alignment，像下图一样：&lt;/p&gt;
&lt;img src=&quot;/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig7.png&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;
&lt;p&gt;上图是英语翻译成法语的一个结果。越亮的地方表示source和target中的words相关性越强（或者说对齐地越准），图中的每一个点的亮度就是前面计算出的权重。&lt;/p&gt;
&lt;p&gt;本文最大的贡献在于提出了attention model，为今后研究对话生成，问答系统，自动文摘等任务打下了坚实的基础。context的定义也成为了一个非常有意思的研究点，rnn是一种思路，cnn同样也是一种思路，简单的word embedding也可以算是一种思路，交叉起来rnn+cnn也可以作为一种思路，将word替换成char可以作为一种思路，思路其实非常多，不同的组合有不同的模型，都可以去探索。&lt;/p&gt;
&lt;p&gt;另外，不知道是不是Yoshua Bengio组的习惯，本文也在附录附上了详细的模型推导过程。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;前面的两篇文章简单介绍了seq2seq在机器翻译领域的尝试，效果令人满意。上一篇也介绍到这一类问题可以归纳为求解P(out
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="attention" scheme="http://rsarxiv.github.io/tags/attention/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/</id>
    <published>2016-06-01T15:15:17.000Z</published>
    <updated>2016-06-07T15:23:12.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;本篇将分享的文章相比于昨天那篇&lt;a href=&quot;http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自于Google，工程性更强一些，学术性有一些不足。本文来自于学术机构，学术范更浓一些。本文的题目是&lt;a href=&quot;http://arxiv.org/pdf/1406.1078.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical&lt;br&gt;  Machine Translation&lt;/a&gt;，作者是来自蒙特利尔大学的&lt;a href=&quot;http://www.kyunghyuncho.me/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Kyunghyun Cho&lt;/a&gt;博士（现在在纽约大学任教），2014年6月登在arxiv上。&lt;/p&gt;
&lt;p&gt;本文最大的两个贡献是：&lt;/p&gt;
&lt;p&gt;1、提出了一种类似于LSTM的GRU结构作为RNN的hidden unit，并且具有比LSTM更少的参数，更不容易过拟合。&lt;/p&gt;
&lt;p&gt;2、较早地（据说2013年就有人提出用seq2seq思路来解决问题）将seq2seq应用在了机器翻译领域，并且取得了不错的效果。&lt;/p&gt;
&lt;p&gt;自然语言生成（NLG）领域中有很多任务，比如机器翻译，&lt;a href=&quot;http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/&quot;&gt;自动文摘&lt;/a&gt;，自动问答，对话生成等，都是根据一个上下文来生成一个文本序列，这里分类两个过程，encoder部分将输入序列表示成一个context，decoder部分在context的条件下生成一个输出序列，联合训练两个部分得到最优的模型。这里的context就像是一个memory，试着保存了encoder部分的所有信息（但往往用较低的维度表示整个输入序列一定会造成信息损失）。本文的思路就是如此，具体可参看下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;本文模型将encoder部分的最后一个hidden state作为context输入给decoder，decoder中的每一个时间t的hidden state s(t)都与context,s(t-1),y(t-1)有关系，而每一个时间t的输出y(t)都与context,s(t),y(t-1)有关。当然，这种模型是非常灵活的，你的context可以有很多种选择，比如可以选encoder中所有hidden state组成的矩阵来作为context，可以用BiRNN计算出两个last hidden state进行拼接作为context；而s(t)和y(t)根据RNN结构不同，也可以将context作为s(0)依次向后传递，而不是每次都依赖于context。&lt;/p&gt;
&lt;p&gt;说完了模型部分，来说说本文最大的贡献是提出了GRU，一种更轻量级的hidden unit，效果还不输LSTM，函数结构如下图：&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig2.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;GRU有两个门函数，reset gate和update gate，公式如下：&lt;/p&gt;
&lt;p&gt;reset gate：&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig3.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;update gate：&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig4.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;reset gate接近于0的时候，当前hidden state会忽略前面的hidden state，在当前输入处reset。reset gate控制了哪些信息可以通过，而update gate控制着多少信息可以通过，与LSTM中的cell扮演着相似的角色。计算出每一步的reset和update gate，即可计算出当前的hidden state，如下：&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig5.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;这里，&lt;/p&gt;
&lt;img src=&quot;/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig6.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;实验部分，作者利用本文模型得到了满意的结果，不再赘述。&lt;/p&gt;
&lt;p&gt;另外，本文在附录部分给出了一个比较详尽的encoder-decoder公式推导过程，大家可以参看原文。&lt;/p&gt;
&lt;p&gt;从context预测output，是一件很神奇的事情。而context又是千变万化的，当下正流行的模型attention model正是在context上做了文章，得到了更好的结果。相信，对context的变化和应用会带来更多好玩的模型。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;本篇将分享的文章相比于昨天那篇&lt;a href=&quot;http://rsarxiv.github.io/2016/05/31/S
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>Sequence to Sequence Learning with Neural Networks #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/</id>
    <published>2016-05-31T15:22:43.000Z</published>
    <updated>2016-06-07T15:23:10.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;seq2seq+各种形式的attention近期横扫了nlp的很多任务，本篇将分享的文章是比较早（可能不是最早）提出用seq2seq来解决机器翻译任务的，并且取得了不错的效果。本文的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1409.3215.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;，作者是来自Google的&lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ilya Sutskever&lt;/a&gt;博士（现在OpenAI）。可以说这篇文章较早地探索了seq2seq在nlp任务中的应用，后续的研究者在其基础上进行了更广泛的应用，比如自动文本摘要，对话机器人，问答系统等等。&lt;/p&gt;
&lt;p&gt;这里看一张很经典的图，如下：&lt;/p&gt;
&lt;img src=&quot;/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;图的左半边是encoder，右半边是decoder，两边都采用lstm模型，decoder本质上是一个rnn语言模型，不同的是在生成词的时候依赖于encoder的最后一个hidden state，可以用下式来表示：&lt;/p&gt;
&lt;img src=&quot;/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig2.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;模型非常简单，就是最普通的多层lstm，实际实现的时候有几点不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用了两种不同的lstm，一种是处理输入序列，一种是处理输出序列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更深的lstm会比浅的lstm效果更好，所以本文选择了四层。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将输入的序列翻转之后作为输入效果更好一些。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里在decoder部分中应用了beam search来提升效果，beam search大概的思路是每次生成词是取使得整个概率最高的前k个词作为候选，这里显然beam size越大，效果越好，但是beam size越大会造成计算的代价也增大，所以存在一个trade off。&lt;/p&gt;
&lt;p&gt;最后通过机器翻译的数据集验证了了seq2seq模型的有效性。&lt;/p&gt;
&lt;p&gt;这里需要讨论的一点是，为什么将输入倒序效果比正序好？文中并没有说，只是说这是一个trick。但后面读了关于attention的文章之后，发现soft attention或者说alignment对于seq2seq这类问题有着很大的提升，我们都知道rnn是一个有偏模型，顺序越靠后的单词在最终占据的信息量越大，那么如果是正序的话，最后一个词对应的state作为decoder的输入来预测第一个词，显然在alignment上来看，这两个词并不是对齐的，反过来，如果用倒序的话，之前的一个词成了最后一个词，在last state中占据了主导，用这个词来预测decoder的第一个词，从某种意义上来说实现了alignment，所以效果会好一些。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;seq2seq+各种形式的attention近期横扫了nlp的很多任务，本篇将分享的文章是比较早（可能不是最早）提出用se
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="seq2seq" scheme="http://rsarxiv.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>大牛主页</title>
    <link href="http://rsarxiv.github.io/2016/05/30/%E5%A4%A7%E7%89%9B%E4%B8%BB%E9%A1%B5/"/>
    <id>http://rsarxiv.github.io/2016/05/30/大牛主页/</id>
    <published>2016-05-31T05:11:49.000Z</published>
    <updated>2016-06-10T22:04:50.000Z</updated>
    
    <content type="html">&lt;p&gt;本篇将汇总PaperWeekly翻译过的作者的主页（持续更新中）：&lt;/p&gt;
&lt;p&gt;1、Konstantin Lopyrev  &lt;b&gt;&lt;a href=&quot;https://github.com/klopyrev&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;2、&lt;a href=&quot;http://people.seas.harvard.edu/~srush/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Alexander M. Rush&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;3、&lt;a href=&quot;https://research.facebook.com/sumit-chopra&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Sumit Chopra&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4、&lt;a href=&quot;http://researcher.watson.ibm.com/researcher/view.php?person=us-nallapati&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ramesh Nallapati&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;5、&lt;a href=&quot;http://researcher.watson.ibm.com/researcher/view.php?person=us-zhou&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Bowen Zhou&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;6、&lt;a href=&quot;http://nlp.stanford.edu/~jpennin/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Jeffrey Pennington&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;7、&lt;a href=&quot;https://research.facebook.com/tomas-mikolov&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tomas Mikolov&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;8、&lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Yoshua Bengio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;9、&lt;a href=&quot;http://www.people.fas.harvard.edu/~yoonkim/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Yoon Kim&lt;/a&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/yoonkim&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;10、&lt;a href=&quot;http://cs.stanford.edu/~quocle/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Quoc Le&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;11、&lt;a href=&quot;http://www.cs.toronto.edu/~rkiros/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ryan Kiros&lt;/a&gt;  &lt;b&gt;&lt;a href=&quot;https://github.com/ryankiros&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;12、&lt;a href=&quot;http://www.cl.cam.ac.uk/~fh295/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Felix Hill&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;13、&lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ilya Sutskever&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;14、[Dzmitry Bahdanau] &lt;b&gt;&lt;a href=&quot;https://github.com/rizar&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;15、&lt;a href=&quot;http://rockt.github.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;TIM ROCKTÄSCHEL&lt;/a&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/rockt&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;16、&lt;a href=&quot;http://www1.icsi.berkeley.edu/~vinyals/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Oriol Vinyals&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;17、&lt;a href=&quot;https://mila.umontreal.ca/en/person/iulian-vlad-serban/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Iulian Vlad Serban&lt;/a&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/julianser&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;18、&lt;a href=&quot;http://web.stanford.edu/~jiweil/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Jiwei Li&lt;/a&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/jiweil&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;19、&lt;a href=&quot;http://homepages.inf.ed.ac.uk/s0681274/About_Me.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Andrew M. Dai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;20、&lt;a href=&quot;http://www.ccs.neu.edu/home/luwang/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lu Wang&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本篇将汇总PaperWeekly翻译过的作者的主页（持续更新中）：&lt;/p&gt;
&lt;p&gt;1、Konstantin Lopyrev  &lt;b&gt;&lt;a href=&quot;https://github.com/klopyrev&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Gi
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Learning Distributed Representations of Sentences from Unlabelled Data #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/</id>
    <published>2016-05-30T20:49:58.000Z</published>
    <updated>2016-06-07T15:23:07.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;sentence representation的文章已经分享了几篇，包括了supervise和unsupervise的方法，但并没有对各种model进行系统地对比和分析，今天分享的这篇文章对现有各种各样的distributed representations of sentences model进行了分类、对比和分析，为了增强对比效果，还提出了两种虚拟的模型。最后将所有的模型在supervised和unsupervised评价任务中进行对比，得出了一些有意义的结论。本文的题目是：&lt;a href=&quot;http://arxiv.org/pdf/1602.03483v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Learning Distributed Representations of Sentences from Unlabelled Data&lt;/a&gt;，作者是来自剑桥大学的&lt;a href=&quot;https://www.cl.cam.ac.uk/~fh295/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Felix Hill&lt;/a&gt;博士。&lt;/p&gt;
&lt;p&gt;首先对现有模型进行了分类描述。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;直接在纯文本上进行训练的模型，模型包括：&lt;a href=&quot;http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/&quot;&gt;Skip-Thought Vector&lt;/a&gt;、&lt;a href=&quot;http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/&quot;&gt;Paragraph Vector&lt;/a&gt;，两种模型都在之前分享过。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在结构化资源上进行训练的模型，这种模型借助了一些纯文本之外的资源进行辅助训练。模型包括：DictRep、CaptionRep、NMT。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DictRep&lt;/code&gt;是本文作者之前提出的一个模型，模型训练了一个从词典定义到预训练好的词向量之间的映射。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CaptionRep&lt;/code&gt;模型架构与DictRep一样，采用的数据集不同而已，这里使用了COCO数据集，训练一个从图像vector representation到图像caption的映射。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NMT&lt;/code&gt;是神经网络机器翻译，该模型架构与skip-thought vector模型相同，但训练数据换成了sentence-aligned翻译文本，WMT语料中的En-Fr和En-De。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文提出的一些新模型。为了解决当前存在模型的问题，本文设计了两种虚拟模型。包括：Sequential (Denoising) Autoencoders(SDAE、SAE)和FastSent。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;SDAE&lt;/code&gt;模型是为了解决Skip-Thought Vector模型对语料中句子连贯性的依赖问题。传统的去噪自编码器（DAE）一般都是一个输入是固定尺寸图像数据的前馈神经网络，本文利用一个噪声函数将传统的DAE扩展到变长度句子中，噪声函数是N(S|p0,px)，S表示一个句子，p0,px都是一个[0,1]的数，表示概率。首先，对于每一个S中的word，N函数会以一个p0的概率来删除word，概率是相互对立的。然后，对于S中的每一对不重叠的bigram，w(i)w(i+1)，N函数会以一个px的概率来交换两个词的位置。最后用一个类似NMT的encoder-decoder模型进行训练，只不过不同的是目标函数变了，变成了使得噪声最小。这里，source是经过噪声函数处理过的sentence，target是原始的sentence。这个模型就是SDAE模型，相比于skip-thought vector，可以处理任意顺序的句子集。如果令px=0,p0=0，我们称为&lt;code&gt;SAE&lt;/code&gt;模型。这里p0其实就是防止深度网络模型训练时过拟合的正则化方法&lt;code&gt;Dropout&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FastSent&lt;/code&gt;模型旨在解决Skip-Thought Vector模型计算速度慢的缺点，解决的思路与word2vec突破传统多层神经网络语言模型的思路类似，只用了一个简单的log-linear层。给定一个用词袋模型表示的句子，模型来预测该句子两边相邻的句子。该模型在训练时也会学习句中每个单词的词向量，并且将句子用句中所有词的词向量之和来表示。&lt;/p&gt;
&lt;p&gt;下图给出了所有模型在性能上的比较：&lt;/p&gt;
&lt;img src=&quot;/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig1.png&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;
&lt;p&gt;其中，OS是指是否需要保留句子在语料中的顺序；R表示需要结构化的训练资源；WO：对词序敏感；SD：句子向量维度；WD：词向量维度；TR：训练时间；TE：编码50w句子需要的时间。&lt;/p&gt;
&lt;p&gt;任务评价一共分为两类，监督学习任务和无监督学习任务。通过大量实验的比较，得出了一下的结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;不同的任务适合不同的表示模型，这听起来像一句废话，也就是说没有哪种模型可以通吃所有的任务。比如：Skip-Thought Vector模型在TREC任务中最好，是因为句子和句子之间的衔接非常好，非常适合这个模型的特点。而Paraphrase detection任务更加适合于SDAE模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;监督学习和无监督学习任务的表现存在差异，在监督学习任务中表现好的模型在无监督学习模型中表现的就会很一般，带有非线性网络结构的Skip Thought Vector、SDAE、NMT模型在监督学习中表现更好，而log-linear类的模型FastSent则在无监督学习任务中表现更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;额外的资源会影响到训练处模型的通用性和实用性，比如一个在线demo需要很快的查询最近邻速度，用fastsent可能就没有问题，但用其他模型就达不到快速的要求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;词序的重要性并没有得到体现。本文的结果给出了一个与常识相左的结论，词序在决定句子意思表示时并没有想象中的那么重要。作者说到，可能是因为当前的评价方式并不能反映出词序的重要性，所以这个问题得不出一个明确的答案。（这点很有意思，在前面分享的一篇文章&lt;a href=&quot;http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/&quot;&gt;How to Generate a Good Word Embedding&lt;/a&gt;中，引用了一个结论，词序信息占了语义信息的20%，那么到底词序对于句子语义有多大的影响？需要好好研究一番）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;评价指标存在缺陷，并不能绝对准确的对比出各个模型的差异。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后，展示一个各模型训练之后的应用效果。&lt;/p&gt;
&lt;img src=&quot;/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig2.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;sentence representation的文章已经分享了几篇，包括了supervise和unsupervise的方法
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>我以为</title>
    <link href="http://rsarxiv.github.io/2016/05/29/%E6%88%91%E4%BB%A5%E4%B8%BA/"/>
    <id>http://rsarxiv.github.io/2016/05/29/我以为/</id>
    <published>2016-05-29T21:09:18.000Z</published>
    <updated>2016-05-29T22:32:58.000Z</updated>
    
    <content type="html">&lt;p&gt;周末了，给自己放一个假，今天不分享paper了，分享一些别的东西。&lt;/p&gt;
&lt;p&gt;一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。于是，在经过几个月内心的煎熬和挣扎，我决定上路了，从此不再为那些不感兴趣、耗时耗力但却没有半点成就感的事情而纠结了，心中犹如放下一块巨石，终于可以静下心来做一些让自己内心感到充实的事情。&lt;/p&gt;
&lt;p&gt;从小生活在一个小县城里，没有那些IT大牛传说一般的经历，小时候爸爸妈妈也并没有给我买电脑，让我成为一个电脑神童，只是希望我快乐地活在这个世界上。第一次接触电脑是小学毕业时的暑假，拿着妈妈给的零花钱在家附近的网吧打CS，那个时候第一次从电脑中获得了一些肤浅的快乐。对“编程”有一种魔性地冲动，是在初二看了一本盗版的比尔盖茨传，里面讲着盖茨各种传奇的故事，用一台电脑创造了一个帝国，当时对他的种种经历都非常崇拜，还跟着传记里的一些描述来模仿他，困了累了的时候开始前后摇晃自己的身体。那个时候讲盖茨视为自己的偶像，是那种真正的偶像，和现在很多女孩迷小鲜肉是一种感觉。那个时候经常会幻想自己有朝一日可以写代码，可以像偶像一样建立一个帝国。然而，梦想在不正确的时间可能只适合放在心里珍藏起来。&lt;/p&gt;
&lt;p&gt;初中毕业之后，考上了省里最好高中之一，来到了省城读书，这个阶段为我打开了一扇很大很宽的门，让我走进了一个更大的世界。在这里接触到了visual basic，Pascal，机器人比赛，感受到了一个更加生动的世界，更加有趣的世界。原来学习可以不只是读书，不只是做题，还可以动手实现一个好玩的机器人，实现一个有趣的程序。三年的高中生活让我看到了一个更好玩的世界。没有辛苦地努力学习，只是因为高三参加的一次奥林匹克竞赛保送到了南方的一所985，因为一些不可控的原因，学习了一个一直想努力感兴趣却一直都没感兴趣的专业，四年除了一些竞赛成绩没有什么其他的亮点。大学的时候，抓住了一切业余时间来多写程序，也跟着刷一些学校自己online judge系统上的题目，但终究不是太系统。在大四下学期的时候，开始疯狂崇拜扎克伯格，尤其是看了《社交网络》之后，大概是因为当时自己的状态很低落，一方面考研成绩并不理想，一方面谈了三年的女朋友选择了更安稳的港湾，整个人的状态一落千丈，非常需要一些提气的东西来鼓励自己走出困境，于是用了一个开源的sns程序，php写的，在大学同学这个圈子里运营了一个社交网络，叫memory。那段时光，我开始自学一些php，每天给网站添加一些小的新功能，来满足同学们的种种好玩的需求，每天忙活到夜里一两点，却感觉到非常充实，非常有成就感。那段时光大概是大学四年里最快乐、最踏实的一段日子了。&lt;/p&gt;
&lt;p&gt;后来，因为种种不可抗拒的原因没有在学校继续深造，去了一个自己也没法选择的单位工作。工作第一年在北京长期出差，有机会接触到了真正的IT圈，看到了真实的创业，真刀真枪，也看到了推荐系统从2011年开始在国内火起来，大大小小的网站都在搜罗推荐系统方面的人才，供不应求，关于推荐系统方面的交流活动也是一个接一个，媒体也在热炒，甚至都说推荐系统有望接管了搜索引擎的地位，现在想想真是可笑。而现在，人工智能处于一个非常火热的状态，有许多家专门报道人工智能相关资讯的媒体在社交网络上也非常活跃，还有大量的自媒体，每天也都在分享着各种各样和人工智能有关的信息。仿佛，真正的人工智能就快要实现了一样，尤其是阿尔法狗事件将人工智能推向了一个新的高度，人才市场上又开始吆喝着，严重缺乏人工智能、自然语言处理、深度学习、计算机视觉等方面的人才，媒体每天也在鼓吹着各个方面的论调，有的甚至在说AI威胁论，很多做其他研究的人也随之变成了人工智能专家，深度学习专家，自然语言处理专家。现在，这个世界上最不缺少的是专家，然后就是看热闹不嫌事大的吃瓜群众。网络上充斥各种形态的网红、大V，掌握着充分的话语权，许多不明真相的群众总是特别迷信他们说的话，形成了一种不健康的氛围。屌丝迷信小V，小V迷信大V，大V迷信大大V，一个人的title远比他的内容更能让人信服；如果学术中，总是这样迷信权威或者迷信title，学术该如何进步？长江后浪该如何推前浪？毕业之后的这些年，我看到了一个更加多元化的世界，一个兼容并包的世界，一个充满机会也充满挑战的世界。&lt;/p&gt;
&lt;p&gt;我以为，一个人的胸怀和他的视野成正比，一个人的视野和他看到的世界有关系，一个人可以通过多旅行，到处看看来拓宽自己的视野，也可以通过多读书来丰富自己的内心世界。&lt;/p&gt;
&lt;p&gt;我以为，人生短暂，不应浪费自己的时间在不感兴趣的事情上，时间宝贵，谁都不应该视别人的时间如粪土，大肆去浪费别人的时间。&lt;/p&gt;
&lt;p&gt;我以为，生活的本质应该是生活本身，我们努力干活，努力拼搏，为的不就是生活中每一点一滴都过的很幸福嘛？荣誉也好、成就也罢终究是过眼云烟，敌不过内心持久的充实，充实并不是谁给你的，而是你自己对自己的一个肯定。&lt;/p&gt;
&lt;p&gt;我以为，幸福就是不打扰到别人的快乐，可能是陪爱人一起看看大海，可能是一起拍拍照，可能是一起遛遛狗，也可能是一起发发呆。幸福也是将一个又一个的心愿完成，也是拥有一辆华丽的车子，尽管可能只是一辆淘宝购物车。&lt;/p&gt;
&lt;p&gt;我以为，每天读一篇paper，写一篇博客，来丰富自己的内心和知识体系也是一件让我快乐和幸福的事情。每天可以有很多好玩的想法，并且可以通过自己的双手来实现这个想法也是一件让我快乐和幸福的事情。&lt;/p&gt;
&lt;p&gt;我以为，编程让我感觉到自己像一个造物主，在程序世界里，每一行代码，每一个变量，每一个函数都是一个活生生的人，协同地一起工作着。&lt;/p&gt;
&lt;p&gt;我以为，简单和纯粹才是这个世界上最真实的元素，纯真才是对人最大的夸奖，而不是什么帅气，漂亮，有才。&lt;/p&gt;
&lt;p&gt;我以为，陪伴才是最长情的告白，感谢我家狗子hare童鞋每一个日日夜夜的陪伴，感谢我的爱人无条件地支持我做我喜欢做的事情，并且真的敢于放弃一些已有的、很好的待遇来跟着我去完成一个梦想。谢谢！&lt;/p&gt;
&lt;img src=&quot;/2016/05/29/我以为/1.jpg&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;周末了，给自己放一个假，今天不分享paper了，分享一些别的东西。&lt;/p&gt;
&lt;p&gt;一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。于是，在经过几个月内心的煎熬和挣扎，我决定上路了，从此不再为那些不感兴趣、耗
    
    </summary>
    
    
      <category term="随笔" scheme="http://rsarxiv.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Skip-Thought Vectors #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/</id>
    <published>2016-05-28T15:51:24.000Z</published>
    <updated>2016-06-07T15:23:04.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;已经分享过多种无监督的word level表示模型，和多种有监督的sentence level表示模型，以及与word2vec模型类似的paragraph vector模型。无监督模型比有监督模型带给大家更多的惊喜，本文将会分享一篇sentence level的无监督表示模型，模型中用到了当下流行的seq2seq框架。paper的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1506.06726v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Skip-Thought Vectors&lt;/a&gt;，作者是来自多伦多大学的&lt;a href=&quot;http://www.cs.toronto.edu/~rkiros/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ryan Kiros&lt;/a&gt;博士。&lt;/p&gt;
&lt;p&gt;word level表示已经有太多无监督模型，然而sentence level表示大多仍停留在监督模型的范畴内，比如之前分享过的RNN、CNN、RCNN等模型来表示一个句子，主要是针对具体的分类任务来构造句子向量，仅适用于本任务，不具有一般性。之前，Tomas Mikolov（word2vec的作者）提出了一种类似于Word2vec的paragraph vector，也是一种无监督模型，但并不能很好地扩展来用。&lt;/p&gt;
&lt;p&gt;本文旨在提出一个通用的无监督句子表示模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。&lt;/p&gt;
&lt;p&gt;当然，这里存在一个很大的问题是，如果测试数据中有未登录词，如何表示这个未登录词？针对这个问题，本文提出了一种词汇表扩展的方法来解决这个问题。&lt;/p&gt;
&lt;p&gt;首先，介绍本文的模型，参考下图来理解：&lt;/p&gt;
&lt;img src=&quot;/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;模型分为两个部分，一个是encoder，一个是两个decoder，分别decode出当前句子的上一句和下一句。&lt;/p&gt;
&lt;p&gt;encoder-decoder框架已经介绍过太多次了，这里不再赘述。本文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hidden state作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。如下式：&lt;/p&gt;
&lt;img src=&quot;/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig2.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;其次，介绍下本文的另一大亮点，词汇表扩展。&lt;/p&gt;
&lt;p&gt;借鉴于Tomas Mikolov的一篇文章&lt;a href=&quot;http://arxiv.org/pdf/1309.4168.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Exploiting Similarities among Languages for Machine Translation&lt;/a&gt;中解决机器翻译missing words问题的思路，对本文训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，本文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。文中给出了一个例子，如下图：&lt;/p&gt;
&lt;img src=&quot;/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig3.png&quot; width=&quot;400&quot; height=&quot;400&quot;&gt;
&lt;p&gt;当然，词汇表扩展有很多方法，比如不同词，而用字符来作为基本元素，这种思路在语言模型中也常常被用到。&lt;/p&gt;
&lt;p&gt;最后，作者在Semantic relateness、Paraphrase detection、Image-sentence ranking和classification任务中进行了测试和对比，验证了本文模型的效果。最后还给出了在多个数据集上对句子聚类的可视化结果，以及用decoder部分生成一段话。&lt;/p&gt;
&lt;p&gt;关于未来的改进，作者有几点想法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用更深的encoder和decoder网络。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用更大的窗口，而不仅仅预测上一句和下一句。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;试着将sentence替换成paragraph。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;换一些别的encoder来做，比如用CNN。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每个想法都可能会是未来另一篇牛paper的思路。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;看过了很多的decoder，有char-level，word-level和sentence-level，我有一个小小的想法是，到底哪种level生成的paragraph更出色呢？速度方面，不必比较了，sentence-level一定要快一些，但是质量方面呢？&lt;/code&gt;文中最后给出了一个本文模型生成的demo，如下：&lt;/p&gt;
&lt;img src=&quot;/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig4.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;本文作者还开源了该模型的实现&lt;a href=&quot;https://github.com/ryankiros/skip-thoughts&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;代码&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;已经分享过多种无监督的word level表示模型，和多种有监督的sentence level表示模型，以及与word2v
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Recurrent Convolutional Neural Networks for Text Classification #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/</id>
    <published>2016-05-27T15:24:12.000Z</published>
    <updated>2016-06-07T15:23:02.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;介绍了CNN表示文本的模型之后，本篇将会分享一篇用CNN结合RNN的模型来表示文本。paper题目是&lt;a href=&quot;http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Recurrent Convolutional Neural Networks for Text Classification&lt;/a&gt;，作者是来自中科院大学的来斯惟博士。&lt;/p&gt;
&lt;p&gt;本文要解决的问题是文本分类，文本分类最关键的问题是特征表示，传统的方法经常会忽略上下文信息和词序，无法捕捉到词义。近几年随着深度学习的火热，研究者们通过借助神经网络模型来解决传统方法存在的问题。比如：Socher提出的Recursive Neural Network（递归神经网络）模型，通过一种树结构来捕捉句子语义，取得了不错的效果，但时间复杂度是O(n2)，并且无法用一棵树来表示两个句子之间的关系。再比如：Recurrent Neural Network（循环神经网络）模型，时间复杂度是O(n)，每个单词的表示都包含了之前所有单词的信息，有很强的捕捉上下文的能力，但该模型有偏，后面的单词比前面的单词更重要，但这与常识并不相符，因为句中关键的词不一定在最后面。为了解决RNN的有偏性问题，有的研究者提出了用CNN（卷积神经网络）来表示文本，并且时间复杂度也是O(n)，但是CNN存在一个缺陷，卷积窗口的大小是固定的，并且这个窗口大小如何设置是一个问题，如果设置小了，则会损失有效信息，如果设置大了，会增加很多的参数。&lt;/p&gt;
&lt;p&gt;于是，针对上述模型存在的问题，本文提出了RCNN（循环卷积神经网络）模型，模型架构图如下：&lt;/p&gt;
&lt;img src=&quot;/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png&quot; width=&quot;600&quot; height=&quot;600&quot;&gt;
&lt;p&gt;首先，构造CNN的卷积层，卷积层的本质是一个BiRNN模型，通过正向和反向循环来构造一个单词的下文和上文，如下式：&lt;/p&gt;
&lt;img src=&quot;/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig2.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;得到单词的上下文表示之后，用拼接的方式来表示这个单词，如下式：&lt;/p&gt;
&lt;img src=&quot;/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig3.png&quot; width=&quot;300&quot; height=&quot;300&quot;&gt;
&lt;p&gt;将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层（max-pooling），即将刚刚得到的所有单词的潜语义向量中每个维度上最大的值选出组成一个新的向量，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。&lt;/p&gt;
&lt;p&gt;为了验证模型的有效性，在四组包括中文、英文的分类任务中进行了对比实验，取得了满意的结果。&lt;/p&gt;
&lt;p&gt;本文灵活地结合RNN和CNN构造了新的模型，利用了两种模型的优点，提升了文本分类的性能。这也提供了一种研究思路，因为每一种model都有其鲜明的优点和无法回避的缺点，如何利用别的model的优点来弥补自身model的缺点，是改进model的一种重要思路。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;介绍了CNN表示文本的模型之后，本篇将会分享一篇用CNN结合RNN的模型来表示文本。paper题目是&lt;a href=&quot;ht
    
    </summary>
    
    
      <category term="CNN" scheme="http://rsarxiv.github.io/tags/CNN/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="RNN" scheme="http://rsarxiv.github.io/tags/RNN/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>How to Generate a Good Word Embedding #PaperWeekly#</title>
    <link href="http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/"/>
    <id>http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/</id>
    <published>2016-05-26T14:17:24.000Z</published>
    <updated>2016-06-07T15:22:59.000Z</updated>
    
    <content type="html">&lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;之前介绍过几种生成word embedding的方法，那么针对具体的任务该如何选择训练数据？如何选择采用哪个模型？如何选择模型参数？本篇将分享一篇paper来回答上述问题，paper的题目是&lt;a href=&quot;http://cn.arxiv.org/pdf/1507.05523.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;How to Generate a Good Word Embedding&lt;/a&gt;，作者是来自中科院大学的来斯惟博士。&lt;/p&gt;
&lt;p&gt;当前，word embedding的模型有很多，性能几乎都是各说纷纭，每个模型在自己选定的数据集和任务上都取得了state-of-the-art结果，导致学术研究和工程应用上难以做出选择。不仅仅在word embedding这个子方向上存在这样的问题，很多方向都有类似的问题，如何公平客观地评价不同的模型是一个很困难的任务。本文作者试着挑战了一下这个难题，并且给出了一些有意义的结果。&lt;/p&gt;
&lt;p&gt;本文所做研究都是一个同一个假设，即：出现在相似上下文的单词具有相似的意思。&lt;/p&gt;
&lt;p&gt;下面来看下不同模型的比较，不同word embedding模型之间主要的区别在于两点：&lt;/p&gt;
&lt;p&gt;1、目标词和上下文的关系&lt;/p&gt;
&lt;p&gt;2、上下文的表示方法&lt;/p&gt;
&lt;p&gt;本文提供探讨了6种模型，并从这两个方面对模型进行了对比，如下图：&lt;/p&gt;
&lt;img src=&quot;/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig1.png&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;
&lt;p&gt;c表示上下文，w表示目标词。首先看w和c的关系，前五种模型均是用c来预测w，只有C&amp;amp;W模型是给w和c的组合来打分。再看c的表示方法，Order模型是本文为了对比增加的一个虚拟模型，考虑了词序信息，将c中每个单词拼接成一个大向量作为输入，而word2vec的两个模型skip-gram和cbow都是将上下文处理为一个相同维度的向量作为输入，其中skip-gram选择上下文中的一个词作为输入，cbow将上下文的几个词向量作了平均，LBL、NNLM和C&amp;amp;W模型都是在Order模型的基础上加了一层隐藏层，将上下文向量做了一个语义组合。具体见下表：&lt;/p&gt;
&lt;img src=&quot;/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig2.png&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;
&lt;p&gt;据研究估计，文本含义信息的20%来自于词序，剩下的来自于词的选择。所以忽略词序信息的模型，将会损失约20%的信息。&lt;/p&gt;
&lt;p&gt;本文做了包括三种类型的八组对比实验，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;研究词向量的语义特性。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy task：semantic和syntactic。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将词向量作为特征。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用词向量来初始化神经网络模型。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford Sentiment Treebank；后者用Wall Street Journal数据集进行了POS tagging任务。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经过大量的对比实验，作者回答了以下几个问题：&lt;/p&gt;
&lt;p&gt;Q：哪个模型最好？如何选择c和w的关系以及c的表示方法？&lt;/p&gt;
&lt;p&gt;A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;amp;W这种将c和w都放在输入层的模型。&lt;/p&gt;
&lt;p&gt;Q：数据集的规模和所属领域对词向量的效果有哪些影响？&lt;/p&gt;
&lt;p&gt;A：数据集的领域远比规模重要，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。&lt;/p&gt;
&lt;p&gt;Q：在训练模型时迭代多少次可以有效地避免过拟合？&lt;/p&gt;
&lt;p&gt;A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果，好的做法是用task data作为early stopping的数据。&lt;/p&gt;
&lt;p&gt;Q：词向量的维度与效果之间的关系？&lt;/p&gt;
&lt;p&gt;A：越大的维度就会有越好的效果，但在一般的任务中50就已经足够了。&lt;/p&gt;
&lt;p&gt;本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果，对今后研究和使用词向量的童鞋们搭建了一个非常坚实的平台。并且在github上开源了&lt;a href=&quot;https://github.com/licstar/compare&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;实验结果&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;工具推荐&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RSarXiv&lt;/code&gt; &lt;b&gt;一个好用的arxiv cs paper推荐系统&lt;/b&gt; &lt;a href=&quot;http://rsarxiv.science/web&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网站地址&lt;/a&gt; &lt;b&gt;ios App下载：App Store 搜索rsarxiv即可获得 &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。&lt;/p&gt;
&lt;img src=&quot;/2016/05/13/Paper翻译列表/qrcode.jpg&quot; width=&quot;350&quot; height=&quot;350&quot;&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;(欢迎大家订阅本博客，订阅地址是&lt;a href=&quot;http://rsarxiv.github.io/atom.xml&quot;&gt;RSS&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;之前介绍过几种生成word embedding的方法，那么针对具体的任务该如何选择训练数据？如何选择采用哪个模型？如何选择
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="word embeddings" scheme="http://rsarxiv.github.io/tags/word-embeddings/"/>
    
  </entry>
  
</feed>
