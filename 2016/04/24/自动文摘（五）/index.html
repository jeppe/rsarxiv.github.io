<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>自动文摘（五） | RSarXiv</title>
  
  
  <meta name="description" content="引
读万卷书 行万里路

最近读了几篇关于deep learning在summarization领域应用的paper，主要的方法是借鉴机器翻译中seq2seq的技术，然后加上attention model提升效果。今天来分享其中一篇paper，Generating News Headlines wi">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="自动文摘（五）"/>
  <meta property="og:site_name" content="RSarXiv"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="RSarXiv" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">RSarXiv</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-24T18:14:12.000Z"><a href="/2016/04/24/自动文摘（五）/">2016-04-24</a></time>
      
      
  
    <h1 class="title">自动文摘（五）</h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>读万卷书 行万里路</strong></p>
</blockquote>
<p>最近读了几篇关于deep learning在summarization领域应用的paper，主要的方法是借鉴机器翻译中seq2seq的技术，然后加上attention model提升效果。今天来分享其中一篇paper，<b>Generating News Headlines with Recurrent Neural Networks</b></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文的思路是用LSTM RNN作为encoder-decoder框架的模型，并且使用了attention模型来生成新闻文章的标题，效果很好。并且提出了一种简化版的attention mechanism，相比于复杂版的注意力机制在解决headline generation问题上有更好的效果。</p>
<p><code>本文定义的文本摘要问题是给新闻文章命题，为了套用seq2seq技术，一般都会将source定义为新闻的第一句话，target定义为标题。本文的亮点在于提出了一种简化版的注意力机制，并且得到了不错的结果。</code></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2016/04/24/自动文摘（五）/model.png" width="400" height="650">
<p>encoder使用文章内容作为输入，一个时间点表示一个单词，每个单词先通过embedding层将词转换为一个分布式向量（<code>word embedding</code>）。每个词向量都由前一个词向量生成，第一个词定义为0向量。</p>
<p>decoder将encoder中最后一个词向量作为输入，decoder本质是一个rnnlm，使用softmax和attention mechanism来生成每个词。</p>
<p>损失函数：</p>
<img src="/2016/04/24/自动文摘（五）/lossfunction.png" width="400" height="650">
<p>这里y是输出的词，x是输入的词。</p>
<p>本文采用了4层LSTM，每层有600个单元，使用Dropout控制过拟合，所有参数的初始值都服从-0.1到0.1的平均分布，训练方法是RMSProp，学习速率0.01，动量项0.9，衰减项0.9，训练9个回合，在第5个回合之后，每个回合都将训练速率减半。batch训练，384组训练数据为一个batch。</p>
<p><code>模型的定义和训练方法都是借鉴于其他文章，模型参数的不同并不是什么创新，别人用gru或者birnn，你用lstm，或者别人用2层，你用3层、4层更多层，不同的模型参数可能会有不同的state-of-the-art结果，但并不会对大家认识abstractive summarization问题有什么实质性的帮助，也不会促进这个领域的发展，只是用着现有的方法在这个领域刷了一篇paper罢了。</code></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>注意力机制可以用来帮助神经网络更好地理解输入数据，尤其是一些专有名词和数字。attention在decoder阶段起作用，通过将输出与所有输入的词建立一个权重关系来让decoder决定当前输出的词与哪个输入词的关系更大（即应该将注意力放到哪个词上）。</p>
<p>本文采用两种不同的注意力机制，第一种称作复杂注意力模型（<code>complex attention</code>），与文献[2]中Minh-Thang采用的点乘机制（<code>dot mechanism</code>）一样，看下图：</p>
<img src="/2016/04/24/自动文摘（五）/complex.png" width="400" height="650">
<p>第二种称作简单注意力模型（<code>simple attention</code>），是第一种模型的变种，该种模型使得分析神经网络学习注意力权重更加容易。看下图：</p>
<img src="/2016/04/24/自动文摘（五）/simple.png" width="400" height="650">
<p>对比两幅图可以看出区别在于隐藏层的最后一层的表示上，简单模型将encoder部分在该层的表示分为两块，一小块用来计算注意力权重（<code>attention weight</code>），另一大块用来作为上下文（<code>context vector</code>）；decoder部分在该层的表示也分为两块，一小块用来计算注意力权重，另一大块用来导入softmax，进行输出预测。</p>
<p><code>simple attention mechanism的提出可以算作本文的主要贡献，但是感觉贡献量并不大。修改所谓的理论模型，而不仅仅是对模型参数进行修改，本质上是对encoder的context vector进行了更换，用了一些技巧，比如文中的方法，将隐藏层最后一层的表示分为两部分，一部分用来表示context，一部分用来表示attention weight，就有了新的模型。</code></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><p>本文用English Gigaword数据集，该数据集包括了六大主流媒体机构的新闻文章，包括纽约时报和美联社，每篇文章都有清晰的内容和标题，并且内容被划分为段落。经过一些预处理之后，训练集包括5.5M篇新闻和236M单词。</p>
<h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>headlines作为target，news text的第一段内容作为source，预处理包括：小写化，分词，从词中提取标点符号，标题结尾和文本结尾都会加上一个自定义的结束标记<code>&lt;eos&gt;</code>，那些没有标题或者没有内容或者标题内容超过25个tokens或者文本内容超过50个tokens都会被过滤掉，按照token出现频率排序，取top 40000个tokens作为词典，低频词用符号<code>&lt;unk&gt;</code>进行替换。</p>
<p>数据集被划分为训练集和保留集，训练集将会被随机打乱。</p>
<p><code>数据的预处理是一件重要的事情，处理的好坏直接影响结果的好坏。本文的每一个处理细节都交代的很清楚，有希望做相同实验的童鞋可以借鉴他的处理方法</code></p>
<h2 id="Dataset-Issues"><a href="#Dataset-Issues" class="headerlink" title="Dataset Issues"></a>Dataset Issues</h2><p>训练集中会出现标题与所输入文本关系不大的情况，比如：标题包括以下字样For use by New York Times service clients，或者包括一些代码，biz-cover-1等等，本文对此不作处理，因为一个理想的模型可以处理这些问题。‘</p>
<p><code>数据集本身会有一些错误，但一个好的模型是可以处理好这些错误的数据，所以本文对此种数据并不做处理。</code></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>模型的优劣用两种方法进行评价。第一种，将训练集和保留集<code>损失值</code>作为评价指标；第二种，将<code>BLEU</code>作为评价指标，为了保证效率，保留集仅仅用了384个样本进行计算。</p>
<p><code>评价指标也是常规的两种，两种数据集上的loss值直观地反应了训练和测试效果，BLEU是机器翻译领域中常用的评价标准。</code></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>计算硬件是GTX 980 Ti GPU，每种模型的计算都会花费4.5天时间。效果直接看下图：</p>
<img src="/2016/04/24/自动文摘（五）/evaluation.png" width="400" height="650">
<p>在应用模型结果做保留集的预测时，不同新闻来源的文章预测效果不一样。比如：在BBC、华尔街日报、卫报的效果就非常好，但是在赫芬顿邮报和福布斯的效果就很差。</p>
<p><code>结果也是看上图也是一目了然，本文的simple attention mechanism更胜一筹。</code></p>
<h2 id="Understanding-information-stored-in-last-layer-of-the-neural-network"><a href="#Understanding-information-stored-in-last-layer-of-the-neural-network" class="headerlink" title="Understanding information stored in last layer of the neural network"></a>Understanding information stored in last layer of the neural network</h2>
      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/nlp/">nlp</a>, <a href="/tags/paper/">paper</a>, <a href="/tags/自动文摘/">自动文摘</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">留言</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://rsarxiv.github.io/2016/04/24/自动文摘（五）/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>7</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>5</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 RSarXiv
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>