<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>第 2 页 | RSarXiv</title>
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="RSarXiv"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="RSarXiv" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">RSarXiv</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-21T04:37:06.000Z"><a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">2016-05-20</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <blockquote>
<p><strong> 站得高，望得远 </strong></p>
</blockquote>
<p>今天分享一篇年代久远但却意义重大的paper，<a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a>。作者是来自蒙特利尔大学的Yoshua Bengio教授，deep learning技术奠基人之一。</p>
<p>本文于2003年第一次用神经网络来解决语言模型的问题，虽然在当时并没有得到太多的重视，却为后来深度学习在解决语言模型问题甚至很多别的nlp问题时奠定了坚实的基础，后人站在Yoshua Bengio的肩膀上，做出了更多的成就。包括Word2Vec的作者Tomas Mikolov在NNLM的基础上提出了RNNLM和后来的Word2Vec。文中也较早地提出将word表示一个低秩的向量，而不是one-hot。word embedding作为一个language model的副产品，在后面的研究中起到了关键作用，为研究者提供了更加宽广的思路。</p>
<p>本文最大的贡献在于用多层感知器（<code>MLP</code>）构造了语言模型，如下图：</p>
<img src="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/arch.png" width="600" height="600">
<p>模型一共三层，第一层是映射层，将n个单词映射为对应word embeddings的拼接，其实这一层就是MLP的输入层；第二层是隐藏层，激活函数用tanh；第三层是输出层，因为是语言模型，需要根据前n个单词预测下一个单词，所以是一个多分类器，用softmax。整个模型最大的计算量集中在最后一层上，因为一般来说词汇表都很大，需要计算每个单词的条件概率，是整个模型的计算瓶颈。</p>
<p>这里，需要注意的是需要提前初始化一个word embedding矩阵，每一行表示一个单词的向量。词向量也是训练参数，在每次训练中进行更新。这里可以看出词向量是语言模型的一个附属品，因为语言模型本身的工作是为了估计给定的一句话有多像人类的话，但从后来的研究发现，语言模型成了一个非常好的工具。</p>
<p>softmax是一个非常低效的处理方式，需要先计算每个单词的概率，并且还要计算指数，指数在计算机中都是用级数来近似的，计算复杂度很高，最后再做归一化处理。此后很多研究都针对这个问题进行了优化，比如层级softmax，比如softmax tree。</p>
<p>当然NNLM的效果在现在看来并不算什么，但对于后面的相关研究具有非常重要的意义。文中的Future Work提到了用RNN来代替MLP作为模型可能会取得更好的效果，在后面Tomas Mikolov的博士论文中得到了验证，也就是后来的RNNLM。</p>
<p>所以说我们赶上了一个好的时代，可以站在巨人的肩膀上，看到更远的未来。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-19T00:56:53.000Z"><a href="/2016/05/18/自动文摘（十三）/">2016-05-18</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/18/自动文摘（十三）/">自动文摘（十三）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 天下武功，唯快不破 </strong></p>
</blockquote>
<p>今天分享的paper是<b>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</b>，作者来自香港大学和华为诺亚方舟实验室。</p>
<p><code>本文的模型通过借鉴人类在处理难理解的文字时采用的死记硬背的方法，提出了COPYNET。将拷贝模式融入到了Seq2Seq模型中，将传统的生成模式和拷贝模式混合起来构建了新的模型，非常好地解决了OOV问题。解决问题的思路与之前的一篇有关Pointer的文章十分类似。decoder部分不断地变复杂，考虑的因素越来越多，模型的效果也越来越好。如果结合上一篇Minimum Risk Training的训练方法，相信在评价指标上会更进一步。</code></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>Seq2Seq技术占据了nlp多个研究任务的评测榜首，包括最早提出该技术的机器翻译，句法分析，文本摘要，对话系统。Seq2Seq本质上是一个encoder-decoder的模型，encoder部分将输入的序列变换成某一种表示，然后decoder将这种表示变换成输出序列。在Seq2Seq的基础上，首次增加注意力机制来做机器翻译的自动对齐。注意力机制在很大程度上提升了Seq2Seq的性能。</p>
<p>本文研究了人类语言交流的另一个机制，“拷贝机制”（<code>copy mechanism</code>），定位到输入序列中的某个片段，然后将该片段拷贝到输出序列中。比如：</p>
<img src="/2016/05/18/自动文摘（十三）/fig1.png" width="400" height="400">
<p>但是注意力机制严重依赖于语义的表示，在系统需要获取到命名实体或者日期时难以准确地表示。相对之下，拷贝机制更加接近于人类处理语言问题的方式。本文提出了COPYNET系统，不仅具备传统Seq2Seq生成词的能力，而且可以从输入序列中拷贝合适的片段到输出序列中。在合成数据和真实数据中均取得了不错的结果。</p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>文章的这部分简单介绍了一下Seq2Seq+Attention Mechanism技术，前面的博客分享了很多这部分的内容，这里就不再赘述了。</p>
<h1 id="COPYNET"><a href="#COPYNET" class="headerlink" title="COPYNET"></a>COPYNET</h1><p>从神经学角度来讲，拷贝机制和人类的死记硬背类似，较少地理解到了意思但保留了字面的完整。从模型的角度来讲，拷贝机制相比于soft注意力模型更加死板，所以更难整合到神经网络模型中。</p>
<h2 id="模型综述"><a href="#模型综述" class="headerlink" title="模型综述"></a>模型综述</h2><p>COPYNET依然是一个encoder-decoder模型，如图1所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig2.png" width="600" height="600">
<p>encoder采用了一个双向RNN模型，输出一个隐藏层表示的矩阵M作为decoder的输入。decoder部分与传统的Seq2Seq不同之处在于以下三部分：</p>
<ul>
<li><b>预测</b>：在生成词时存在两种模式，一种是生成模式，一种是拷贝模式，生成模型是一个结合两种模式的概率模型。</li>
<li><b>状态更新</b>：用t-1时刻的预测出的词来更新t时刻的状态，COPYNET不仅仅词向量，而且使用M矩阵中特定位置的hidden state。</li>
<li><b>读取M</b>：COPYNET也会选择性地读取M矩阵，来获取混合了内容和位置的信息。</li>
</ul>
<h2 id="拷贝模式和生成模式"><a href="#拷贝模式和生成模式" class="headerlink" title="拷贝模式和生成模式"></a>拷贝模式和生成模式</h2><p>首先，构造了两个词汇表，一个是高频词词汇表，另一个是只在输入序列中出现过一次的词，这部分的词用来支持COPYNET，用UNK表示超纲词（OOV），最终输入序列的词汇表是三者的并集。</p>
<p>给定了decoder当前状态和M矩阵，生成目标单词的概率模型如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig3.png" width="300" height="300">
<p>模型中g表示生成模式，c表示拷贝模式。两种模式的概率由下式给定：</p>
<img src="/2016/05/18/自动文摘（十三）/fig4.png" width="300" height="300">
<p>共四种可能情况，下图会更形象一些：</p>
<img src="/2016/05/18/自动文摘（十三）/fig5.png" width="300" height="300">
<p>其中生成模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig6.png" width="300" height="300">
<p>拷贝模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig7.png" width="300" height="300">
<h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><p>decoder状态更新的公式是</p>
<img src="/2016/05/18/自动文摘（十三）/fig8.png" width="300" height="300">
<p>不同的是这里的t-1时刻的y由下式表示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig9.png" width="300" height="300">
<p>后面的部分是M矩阵中与t时刻y相关的状态权重之和，如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig10.png" width="300" height="300">
<h2 id="M矩阵"><a href="#M矩阵" class="headerlink" title="M矩阵"></a>M矩阵</h2><p>M矩阵中既包含了内容（语义）信息，又包含了位置信息。COPYNET在attentive read时由内容（语义）信息和语言模型来驱动，即生成模式；在拷贝模式时，由位置信息来控制。</p>
<p>位置信息的更新方式如下图所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="300" height="300">
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>一共分为三个实验：</p>
<ul>
<li>简单规则构造的合成数据。</li>
<li>文本摘要相关的真实数据。</li>
<li>简单对话系统的数据。</li>
</ul>
<p>这里只看文本摘要实验。</p>
<h2 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h2><p>数据采用LCSTS中文短文本摘要数据集，分为两个level来测试：word-level和char-level，并且以LCSTS的baseline作为对比，结果如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="400" height="400">
<p>本文的模型远远优于baseline，而且word-level的结果比char-level更好，这与当时LCSTS paper中的结论不同，一个可能的原因是，数据集中包含了大量的命名实体名词（entity name），LCSTS paper中的方法并不能很好地处理大量的UNK单词，因此baseline中的char-level效果比word-level更好，而本文的模型的优势在于处理OOV问题，所以word-level结果更好一些。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1603.06393v2.pdf" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning Training</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p><code>PaperWeekly</code>，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-17T23:24:26.000Z"><a href="/2016/05/17/自动文摘（十二）/">2016-05-17</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/17/自动文摘（十二）/">自动文摘（十二）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 心是自由的，世界就是自由的 </strong></p>
</blockquote>
<p>今天分享的paper是<b>Neural Headline Generation with Minimum Risk Training</b>。</p>
<p><code>本文通过将评价指标融入目标函数来训练模型，在中文和英文数据集上均取得了超过之前所有模型的结果。结果一点也不意外，因为传统的MLE并不是以ROUGE评价指标最大为目标函数，而本文的方法针对了评价指标来做文章，一定会得到不错的结果。反过来，我们需要思考一个问题，如果文本摘要领域中出现了一个更加科学和准确的评价指标，不仅仅简单的比共现n-gram，那么本文的模型会不会得到一个优于其他模型的结果呢？个人觉得本文的方法很好地利用了评价指标，但对于研究摘要问题的本质并无太多的帮助，只是获得了更好的指标。有一点投其所好的感觉。</code></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>本文研究<code>NHG</code>(Neural Headline Generation)模型。</p>
<p>用Neural的思路来解决HG问题有以下优势：</p>
<p>1、完全数据驱动，不依赖与人工标注和语言学特征。</p>
<p>2、完全端到端，引入注意力机制会得到更好的效果。</p>
<p>存在以下弊端：</p>
<p>1、当前的优化方法都是用最大似然估计（<code>MLE</code>）来训练数据，没有将评价指标考虑在内。</p>
<p>本文用Minimum Risk Training(<code>MRT</code>)来改善NHG模型，将评价指标考虑在优化目标内，在中文和英文两个真实数据集上取得了不错的结果。</p>
<h1 id="NHG模型"><a href="#NHG模型" class="headerlink" title="NHG模型"></a>NHG模型</h1><img src="/2016/05/17/自动文摘（十二）/fig1.png" width="400" height="400">
<p>模型采用encoder-decoder框架，encoder和decoder都采用rnn作为模型。</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder包括两种：<code>GRU</code>和<code>Bi-RNN</code>。</p>
<p>Bi-RNN克服了传统RNN的语义偏置最后一个词的缺点。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder采用GRU，在生成结果时引入了注意力模型。</p>
<h1 id="MRT-NHG"><a href="#MRT-NHG" class="headerlink" title="MRT+NHG"></a>MRT+NHG</h1><h2 id="MLE"><a href="#MLE" class="headerlink" title="MLE"></a>MLE</h2><p>传统的训练方法都是采用最大似然估计来做，目标函数如下：</p>
<img src="/2016/05/17/自动文摘（十二）/fig2.png" width="300" height="300">
<h2 id="MRT"><a href="#MRT" class="headerlink" title="MRT"></a>MRT</h2><p>本文采用了最小风险训练方法来训练模型，目的是减少期望的损失。目标函数如下：</p>
<img src="/2016/05/17/自动文摘（十二）/fig3.png" width="300" height="300">
<p>进一步可以推出：</p>
<img src="/2016/05/17/自动文摘（十二）/fig4.png" width="300" height="300">
<p>作进一步近似处理：</p>
<img src="/2016/05/17/自动文摘（十二）/fig5.png" width="300" height="300">
<p>公式中的<img src="/2016/05/17/自动文摘（十二）/fig6.png" width="100" height="100">用来计算误差，这样训练处的模型将会将评价指标考虑在内。ROUGE是最常见的评价方法，所以本文考虑将ROUGE评价方法融入到目标函数中。</p>
<h2 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h2><p>本文考虑两种ROUGE指标，ROUGE-N和ROUGE-L。本文为了将ROUGE评价指标融入到目标函数中，定义了</p>
<img src="/2016/05/17/自动文摘（十二）/fig7.png" width="200" height="300">
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>DUC2004评测对比结果：</p>
<img src="/2016/05/17/自动文摘（十二）/t1.png" width="400" height="400">
<p>英文数据集上，本文模型的结果明显优于其他模型，包括之前的ABS+模型。</p>
<p>中文LCSTS数据上平尺对比结果：</p>
<img src="/2016/05/17/自动文摘（十二）/t2.png" width="400" height="400">
<p>采用MRT目标函数的模型远优于MLE作为目标函数的模型。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1604.01904.pdf" target="_blank" rel="external">Neural Headline Generation
with Minimum Risk Training</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p><code>PaperWeekly</code>，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-14T05:12:01.000Z"><a href="/2016/05/13/Paper翻译列表/">2016-05-13</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/13/Paper翻译列表/">Paper翻译列表</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 读万卷书，行万里路 </strong></p>
</blockquote>
<p>最近精读了多篇自动文摘方面的paper，并且写成了博客，感觉受益良多。接下来会读更多的好paper，并且以一种更加精炼的形式摘译和评价。如果你有兴趣可以扫码关注微信账号：</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<h1 id="Accomplised"><a href="#Accomplised" class="headerlink" title="Accomplised"></a>Accomplised</h1><ul>
<li><b>Generating News Headlines with Recurrent Neural Networks</b></li>
<li><b>A Neural Attention Model for Abstractive Sentence Summarization </b></li>
<li><b>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</b></li>
<li><b>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</b></li>
<li><b>AttSum-Joint Learning of Focusing and Summarization with Neural Attention</b></li>
<li><b>LCSTS: A Large Scale Chinese Short Text Summarization Dataset</b></li>
</ul>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ul>
<li><b>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</b></li>
<li><b>Neural Headline Generation with Minimum Risk Training</b></li>
<li><b>Toward Abstractive Summarization Using Semantic Representations</b></li>
<li><b>Better Summarization Evaluation with Word Embeddings for ROUGE</b></li>
</ul>
<h1 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h1><p>在完成这文章的翻译和评价之后，会继续读nlp各顶级会议每年的best paper，一方面拓宽自己的视野，另一方面分享给同样感兴趣的童鞋。如果你也有兴趣做这件事情，可以发邮件联系我。（rsarxiv@163.com）</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-12T22:38:09.000Z"><a href="/2016/05/12/自动文摘（十一）/">2016-05-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/12/自动文摘（十一）/">自动文摘（十一）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 除了诗和远方，还有眼前的评价方法 </strong></p>
</blockquote>
<p>之前的博客中提到过影响一个技术发展的重要因素有几个，其中一个就是评价方法。因为评价方法是牵引，是参考，是衡量各个模型孰优孰劣的尺子。评价指标是否科学可行直接影响着这个领域能否进入一个正常的研究方向，目前在文本摘要任务中最常用的评价方法是<code>ROUGE</code>（<b>Recall-Oriented Understudy for Gisting Evaluation</b>，最细节的内容可参见Lin,2003的paper <b>Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</b>）。</p>
<p>10多年前提出的方法至今都是自动文摘系统评价的主流方法，不是说它没有缺点，而是至今没有提出一个更加好的评价方法来取代它。</p>
<p>ROUGE收到了机器翻译自动评价方法BLEU的启发，不同之处在于，采用召回率来作为指标。基本思想是将模型生成的摘要与参考摘要的n元组贡献统计量作为评判依据。这里，不同具体任务的参考文献获取方法不同，比如：DUC2003文本摘要中的参考摘要是由人工给出的，而seq2seq模型在处理Gigaword数据集时，用news headline作为摘要，中文微博短文本摘要数据集（哈工大）的参考摘要都是包含在微博内容中。</p>
<h1 id="ROUGE评价指标"><a href="#ROUGE评价指标" class="headerlink" title="ROUGE评价指标"></a>ROUGE评价指标</h1><h2 id="ROUGE-N"><a href="#ROUGE-N" class="headerlink" title="ROUGE-N"></a>ROUGE-N</h2><p>这个指标计算生成的摘要与相应的参考摘要的n-gram召回率。</p>
<h2 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h2><p>这个指标匹配两个文本单元之间的最长公共序列（LCS，Longest Common Subsequence）。</p>
<h2 id="ROUGE-W"><a href="#ROUGE-W" class="headerlink" title="ROUGE-W"></a>ROUGE-W</h2><p>这个指标计算加权的LCS。</p>
<h2 id="ROUGE-S"><a href="#ROUGE-S" class="headerlink" title="ROUGE-S"></a>ROUGE-S</h2><p>计算跳二元组（skip-bigram）同现统计量。</p>
<p>ROUGE自动评测方法最大的优点是不依赖语言处理工具，缺点是死板，不够灵活，没有考虑语义层次上的匹配。可以考虑用word embedding来做语义层次上的评判，而不仅仅是n-gram的匹配。</p>
<h1 id="ROUGE使用方法"><a href="#ROUGE使用方法" class="headerlink" title="ROUGE使用方法"></a>ROUGE使用方法</h1><p>在实际使用时，用ROUGE的开源程序，perl写的脚本，根据提示操作即可。通常用ROUGE-1,ROUGE-2指标来评测文摘的效果。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://research.microsoft.com/en-us/people/cyl/naacl2003.pdf" target="_blank" rel="external">Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</a></p>
<p>[2] <a href="http://www.berouge.com/Pages/default.aspx" target="_blank" rel="external">ROUGE开源工具包</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-12T17:20:59.000Z"><a href="/2016/05/12/自动文摘（十）/">2016-05-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/12/自动文摘（十）/">自动文摘（十）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 生活不只是眼前的苟且，还有paper和远方 </strong></p>
</blockquote>
<p>不知不觉坚持写自动文摘系列的博客已经50天了，本篇是系列的第十篇。其实说是系列文章并不准确，只是每篇博客与自动文摘有关系，但相互之间并没有递进的关系，只是get到了一些点顺手写下来，又懒得起一些好听的名字，所以就简单地命名为系列博客。我不知道这个系列可以写到几，但探索自动文摘技术并不会停止下来。言归正传，最近读了些paper，觉得UNK问题是一个值得关注的问题，所以本文简单讨论一下UNK问题。</p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>UNK是Unknown Words的简称，在用seq2seq解决问题上经常出现，比如机器翻译任务，比如文本摘要任务。在decoder中生成某个单词的时候就会出现UNK问题。decoder本质上是一个语言模型，而语言模型本质上是一个多分类器，通过计算词汇表中的每个单词在当前条件下出现的概率，来生成该条件下的单词。为了提高计算效率，往往只选择出现频次最高的Top N个单词作为词汇表，其他的单词都用UNK来替换，这样导致decoder的时候会出现UNK。其中，很多UNK可能都是一些不常出现的但有意义的词，比如机构名、地名。</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>解决方法有以下几个：</p>
<h2 id="Char-based"><a href="#Char-based" class="headerlink" title="Char-based"></a>Char-based</h2><p>英文字符的种类远远少于词的种类，用char来代替word可以将decoder输出层的维度降低很多，而且覆盖了所有的可能，从根本上避免了UNK的问题。在文本摘要任务中，数据中往往包括很多的人名、地名，基于word来做词汇表的话，经常会在摘要中看到大量的UNK，用基于char的模型来做，会得到不错的效果。</p>
<p>char方法虽然缓解了output部分的计算压力，却将大量计算耗在了input部分，尤其是在处理英文问题时，会将input放大很多倍。而且处理中文问题也不太有优势，常用汉字也有3000左右的规模。</p>
<h2 id="Vocabulary-expansion"><a href="#Vocabulary-expansion" class="headerlink" title="Vocabulary expansion"></a>Vocabulary expansion</h2><p>词汇表扩展的方法，在高频词词汇表中添加一定数量的UNK，并且编号。通过word embedding计算出带编号UNK的第一层最邻近words，如果匹配的这些词在原来词汇表中，则为有效词。有效词越多，本质上词汇表虽然规模没有增加，但表达能力会越强，在decoder生成词时遇到UNK就可以用词汇表中的高频词来替换。</p>
<p>这个方法是一个辅助技巧，可以提升效果，但不会解决根本问题。</p>
<h2 id="Output-layer-boost"><a href="#Output-layer-boost" class="headerlink" title="Output layer boost"></a>Output layer boost</h2><p>这个方法的思路是想办法提升输出层的效率，原始的方法是softmax，这是消耗计算资源的根源。有比如</p>
<p>1、importance sampling : (Bengio et al., 2003)</p>
<p>2、uniform sampling of ranking criterion : (Collobert et al., 2011)</p>
<p>3、hierarchical softmax : (Morin et al., 2005) </p>
<p>4、hierarchical log-bilinear model : (Mnih et al., 2009) </p>
<p>5、structured output layer : (Le et al., 2011)</p>
<p>6、noise-constrastive estimation : (Mnih et al., 2012)</p>
<p>各种各样的方法来提升多分类问题的效率。效率高了，词汇表中就可以放入更多的单词，但治标不治本，只能说改善了效果。</p>
<h2 id="Pointing-Copy"><a href="#Pointing-Copy" class="headerlink" title="Pointing/Copy"></a>Pointing/Copy</h2><p>观察人工参考摘要时会发现，摘要中有很多词都是来自于输入部分，比如机构名、地名、人名。这些词出现很少有的甚至只出现一次，如果靠语言模型来生成是不可能的。基于这个现象，有几篇paper提出用Pointing/Copy机制来生成摘要，两种模型意思上茶太不多，在decoder中存在两种模型来生成单词，一种是常规的生成方式，另一种就是拷贝方式。拷贝模型在很大程度上解决了UNK的问题，rare words都直接用原文中的词放在摘要的相应位置上。</p>
<p>本方法从正面解决了UNK问题，而且计算效率上可能比char-based的方法更好一些，因为并没有引入太大规模的input数据，output部分规模也不大。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-11T14:14:52.000Z"><a href="/2016/05/11/自动文摘（九）/">2016-05-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/11/自动文摘（九）/">自动文摘（九）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>坚持下去就是胜利。</strong></p>
</blockquote>
<p>今天分享一篇关于构造自动文摘数据集的paper，数据集的质量、内容和规模都是直接影响deep learning效果的最直接因素，作用非常重要。题目是<b>LCSTS: A Large Scale Chinese Short Text Summarization Dataset</b>。</p>
<p><code>本文最大的贡献在于构建了一个大规模、高质量中文短文本摘要数据集，弥补了这个空缺。并且在数据集的基础上用了最简单seq2seq给出了一个baseline，为后人的研究提供了基础。从本文的模型中可以看出，unk问题在文本摘要任务中的重要性，如何解决unk问题是提升摘要系统性能的一个重要方向。本文给出了一个思路，用character-based model来绕过这个问题，看过的paper中还有其他的解决思路，后面的博客将会专门介绍unk这个问题。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>自动文本摘要是一个非常难的问题，部分原因是因为缺乏大规模的高质量数据集。本文将会介绍一个取自于新浪微博的大规模中文短文本摘要数据集，数据集中包含了200万真实的中文短文本数据和每个文本作者给出的摘要。同时我们也手动标注了10666份文本的摘要。基于本数据集，我们测试了用RNN来生成摘要得到了不错的效果，不仅仅亚验证了数据集的有效性，而且为今后的研究提供了基准。</p>
<p><code>数据集一直都是困扰deep learning技术更好地应用在各大领域的一大瓶颈，尤其是中文数据集的匮乏，本文工作的意义在于给研究中文自动文摘的学者提供了数据支持。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><img src="/2016/05/11/自动文摘（九）/weibo.png" width="500" height="500">
<p>本文中的数据类似于上图中的形式。</p>
<p>传统研究abstractive summarization的方法将摘要过程分类两步，第一步是使用无监督方法或者语言知识将关键文本提取出来；第二步是用语言规则或者文本生成技术将第一步的结果转述（<code>paraphrase</code>）。近期研究表明深度学习技术有很强地表示学习能力和语言生成能力，尤其是用GPU在大规模数据集上进行计算，许多学者将该技术应用在abstractive summarization任务中。</p>
<p>然而，公开的高质量大规模文本摘要数据集还是少之又少，DUC、TAC、TREC的数据仅仅包括几百条英文人工摘要数据，这种情况在中文环境中更加糟糕。所以本文构建了一个大规模中文短文本摘要数据集。</p>
<p>以下是本文的贡献：</p>
<p>1、构建了目前最大的一个中文短文本摘要数据集。</p>
<p>2、提供了数据集分割的标准方法。</p>
<p>3、研究了数据集的特性，采样10666条样本，并进行数据集的质量评价。</p>
<p>4、利用了基于encoder-decoder rnn技术来生成摘要，作为该任务的基准。</p>
<p><code>大规模数据集的构建往往来自于网络爬虫，获取到了大量的raw data之后，如何处理得到高质量的内容是关键。本文通过抽样选出样本对数据集质量进行评价，并且用当下最流行的seq2seq技术给出了本数据集的benchmark，以供后人研究超越。</code></p>
<h1 id="Data-Collection"><a href="#Data-Collection" class="headerlink" title="Data Collection"></a>Data Collection</h1><p>为了保证质量，我们仅抓取通过认证组织的微博，这些微博更加清楚、规范和有信息。流程具体如下图：</p>
<img src="/2016/05/11/自动文摘（九）/arch.png" width="500" height="500">
<p>1、首先收集50个流行的官方组织用户作为种子。分别来自政治、经济、军事、电影、游戏等领域，比如人民日报。</p>
<p>2、然后从种子用户中抓取他们关注的用户，并且将不是大V，且粉丝少于100万的用户过滤掉。</p>
<p>3、然后抓取候选用户的微博内容。</p>
<p>4、最后通过过滤，清洗，提取等工作得到最后的数据集。</p>
<p><code>这个环节涉及到的技术是爬虫技术，整体的思路比较简单，先选择一些高质量的用户作为起点，从他们关注的用户中过滤出类似的大V用户，然后再爬取所有候选用户的微博内容，清洗、过滤和提取有效数据。</code></p>
<h1 id="Data-Properties"><a href="#Data-Properties" class="headerlink" title="Data Properties"></a>Data Properties</h1><p>数据集主要分为三个部分，如下表：</p>
<img src="/2016/05/11/自动文摘（九）/table.png" width="500" height="500">
<p>1、第一部分是本数据集的主要部分，包含了2400591对（短文本，摘要），这部分数据用来训练生成摘要的模型。</p>
<p>2、第二部分包括了10666对人工标注的（短文本，摘要），每个样本都打了1-5分，分数是用来评判短文本与摘要的相关程度，1代表最不相关，5代表最相关。这部分数据是从第一部分数据中随机采样出来的，用来分析第一部分数据的分布情况。其中，标注为3、4、5分的样本原文与摘要相关性更好一些，从中也可以看出很多摘要中会包含一些没有出现在原文中的词，这也说明与句子压缩任务不同。标注为1、2分的相关性差一些，更像是标题或者是评论而不是摘要。统计表明，1、2分的数据少于两成，可以用监督学习的方法过滤掉。</p>
<p>3、第三部分包括了1106对，三个人对2000对进行了评判，这里的数据独立于第一部分和第二部分。选择3分以上的数据作为短文本摘要任务的测试数据集。</p>
<p><code>数据集的构造是一个非常大的工作，因为涉及到大量的人工标注工作，如何保证所用的训练集、测试集都有很高的质量是一个问题。本文对第一部分的数据做了采样分析，用第二部分数据作为训练高质量数据的样本，提取出了更高质量的训练集，第三部分提供了测试集。到此，数据集比较完整了。</code></p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>本文用了当下流行的seq2seq技术来做验证实验，用第一部分的数据作为训练集，第三部分的3分以上数据作为测试集。一共用了两种方法来处理数据：</p>
<p>1、基于汉字的方法(character-based)，将词汇表降维到了4000。</p>
<p>2、基于词的方法（word-based），本文用jieba做分词，词汇表维度为50000。</p>
<p>两种网络架构：</p>
<p>1、RNN作为Encoder，用最后一个hidden state作为Decoder的输入。</p>
<img src="/2016/05/11/自动文摘（九）/fig1.png" width="500" height="500">
<p>2、Encoder中的所有hidden state的组合作为Decoder的输入。</p>
<img src="/2016/05/11/自动文摘（九）/fig2.png" width="500" height="500">
<p>RNN隐藏层用GRU，生成摘要时用beam search，beam size设置为10。对比结果如下：</p>
<img src="/2016/05/11/自动文摘（九）/result.png" width="500" height="500">
<p>评测方法采用ROUGE-1，ROUGE-2，ROUGE-L，由于标准的ROUGE包是用来评测英文的，所以这里将中文汉字转换成id。结果中基于汉字的RNN context模型有更好的效果。简单分析下原因，基于词的模型由于词汇表的限制，非常容易遇到unknown words，而基于字则不同，可以轻松解决unk的问题。</p>
<p><code>本文的两种模型搭配两种网络架构，涵盖了简单的seq2seq和seq2seq+attention，很明显地可以看到基于字的模型效果更加好，因为成功地避免了unk的问题。最近有的文章在解决unk的问题，比如用了Pointer/Copy Mechanism来解决。下一次要好好总结一下unk问题的解决方案和ROUGE评测方法。</code></p>
<h1 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h1><p>未来工作：</p>
<p>1、多层次RNN。</p>
<p>2、unk的改进。</p>
<p><code>本文的数据集中输入部分并非只有一句话，而是一段话，简单的rnn并不能准确捕捉其意思。unk是一个很棘手的问题，接下来的博客会单独介绍unk的问题。</code></p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-10T23:50:08.000Z"><a href="/2016/05/10/自动文摘（八）/">2016-05-10</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/10/自动文摘（八）/">自动文摘（八）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>最近读的几篇paper都是用seq2seq的思路来解决问题，有点读厌烦了，今天换个口味。分享一篇extractive式的paper，<b>AttSum: Joint Learning of Focusing and Summarization with Neural Attention</b>，AttSum是本文所阐述的摘要系统的名称。</p>
<p><code>本文用了CNN+word embedding来表示sentence，然后将sentence vector加权求和作为document vector，通过将sentence和document映射到同一空间中，更容易在语义层上计算相似度。CNN之前多用于CV领域，后来在NLP中也应用了起来，尤其是各种各样的sentence classification任务中。在这个层面上将deep learning应用到了extractive summarization中，与之前seq2seq的paper有着本质的区别。整体来看，本文并没有太出众的创新点和突出的结果，反倒是提到了Attention机制，但并没有从模型体现地很充分，所以有炒概念的嫌疑。将文本中表示文本的方法应用在seq2seq的encoder部分，是本文的一种扩展和未来要做的工作。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>基于查询的抽取式文本摘要系统通常需要解决好相关性和显著性两个方面的任务，但一般的系统通常将两者分开考虑。而本文将两个任务合并考虑。本文的算法可以自动学习sentence和document cluster的embedding，并且当查询给定之后，可以应用注意力机制来模拟人类阅读行为。算法的有效性在DUC基于查询的文本摘要任务中进行了验证，得到了有竞争力的结果。</p>
<p><code>本文是将最近比较火的注意力模型应用到了extractive文摘系统中，同时也用了sentence embedding来解决语义层面的相关性问题，并没有像之前的文章在改动seq2seq+attention的细节上做文章，而是切换到了另外一种思路上。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>基于查询的文本摘要系统一般应用于多文档摘要任务，既需要考虑摘要中句子的相关性，又要考虑句子的显著性，相关性反应了文档的主题思想，显著性避免了重复和冗余。很长一段时间，逻辑斯特回归是解决这类问题的热门方法，但是类似的大多数的监督学习方法都是将两个问题分开考虑，通过一些feature对相关性和显著性分开打分排序。但是人在写摘要的时候，往往会将相关性和显著性合并起来考虑。</p>
<p>另外，相关性打分的方法也存在弊端，用类似TF-IDF的指标来打分有时并不能得到非常相似的结果，尽管可能匹配到了核心词，但检索出的结果并不一定可以满足用户的要求。</p>
<p>用深度学习的技术来表示feature，会比用简单的feature去打分排序更加科学，将两个指标融合在一个模型中解决文本摘要问题将会是一个不错的方案。本文提出了一个名叫AttSum的文摘系统，联合了相关性和显著性两个指标。</p>
<p>注意力模型已经成功应用在学习多模态对齐问题中，也可以借鉴到本问题当中。人类总是会将注意力放在他们query的东西上面。</p>
<p>本文在DUC2005-2007基于查询的文摘任务中进行了验证测试，在没有使用任何人为feature的情况下，获得了有竞争力的结果。本文的贡献有两点：</p>
<p>1、应用了注意力机制在基于查询的文摘任务中。</p>
<p>2、提出了一种联合查询相关性和句子显著性的神经网络模型。</p>
<p><code>相关性的打分问题是搜索引擎的基本问题，传统的方案是用一些简单的feature，比如TF-IDF来打分排序，但经常会得不到满意的结果，原因在于feature太过肤浅，并没有考虑语义层面的东西，换句话说并没有真正理解用户需要查什么。当然，有一段时间推荐系统扮演着搜索引擎助手的角色，当一个用户通过留下一些蛛丝马迹给网站，网站就会给他做一些个性化的推荐来辅助搜索引擎，但并不能从根本上解决这个问题。于是本文用了sentence embedding，document embedding来解决这个问题，就像当初的word embedding 一样，将语义映射到一个空间中，然后计算距离来测量相关性。</code></p>
<h1 id="Query-Focused-Sentence-Ranking"><a href="#Query-Focused-Sentence-Ranking" class="headerlink" title="Query-Focused Sentence Ranking"></a>Query-Focused Sentence Ranking</h1><img src="/2016/05/10/自动文摘（八）/arch.png" width="600" height="600">
<p>AttSum系统一共分为三层：</p>
<p>1、CNN Layer，用卷积神经网络将句子和query映射到embedding上。</p>
<p>2、Pooling Layer，用注意力机制配合sentence embeddings构造document cluster embeddings。</p>
<p>3、Ranking Layer，计算sentence和document cluster之间的相似度，然后排序。</p>
<h2 id="CNN-Layer"><a href="#CNN-Layer" class="headerlink" title="CNN Layer"></a>CNN Layer</h2><p>这一层的输入是用word embeddings构造的sentence matrix，然后在该矩阵上用一个卷积filter，之后再应用一个max pooliing获取到features，得到输出。</p>
<p><code>这个方法非常简单，是一个典型的CNN的应用，需要注意的是filter的宽度和词向量的宽度一致，看起来和n-gram类似，但是用了卷积神经网络来捕捉sentence matrix中的最大特征。将变长的句子都统一映射到同一个空间中，为后续计算相似度提供了极大的方便。</code></p>
<h2 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h2><p>这一层用sentence embedding加权求和来得到document cluster embedding。首先计算句子和query的相关性：</p>
<img src="/2016/05/10/自动文摘（八）/formula1.png" width="300" height="300">
<p>这里的相关性计算和相似度是两回事，其中M矩阵是一个tensor function，可以用来计算sentence embedding和query embedding之间的相互影响，两个相同的句子会得到一个较低的分数。然后加权求和得到document cluster embedding：</p>
<img src="/2016/05/10/自动文摘（八）/formula2.png" width="300" height="300">
<p>这里，sentence embedding扮演两个角色，既是pooling项，又是pooling权重。一方面，如果一个句子与query高度相关，则权重会很大；另一方面，如果一个句子在文档中是显著的，该句子的embedding也应被表示在其中。</p>
<p>本文强调了attention机制在Rush等人的工作中依赖于手工feature，不是那么自然地模拟人类的注意力，而本文是真正地无干预地在模拟人类的注意力。</p>
<p><code>感觉这一层的模型中只有M比较神秘一些，但整体来说思路还是非常简单，sentence表示出来了，document用sentence加权求和的方式来表示。只是说权重的计算方法很玄乎，还鄙视了其他人在用attention机制时并不自然。</code></p>
<h2 id="Ranking-Layer"><a href="#Ranking-Layer" class="headerlink" title="Ranking Layer"></a>Ranking Layer</h2><p>打分排序层同样简单，用了余弦公式来计算sentence和document之间的相似度。在训练的过程中用了<code>pairwise ranking strategy</code>，选择样本的时候，用了ROUGE-2计算了所有句子的score，高分的作为正样本，低分的作为负样本。</p>
<p>根据pairwise ranking的标准，相比于负样本，AttSum应该给正样本打出更高的分数，因此损失函数定义如下：</p>
<img src="/2016/05/10/自动文摘（八）/formula3.png" width="300" height="300">
<p>训练方式采用mini-batch SGD。</p>
<p><code>排序层也没什么特别的地方，用了最简单的余弦公式来计算相似度，通过结对排序的方法，先用ROUGE-2指标将所有的句子进行了打分，高分的句子作为正样本，低分的作为负样本，构造损失函数，让正样本的分数尽可能高，负样本的分数尽可能低。</code></p>
<h1 id="Sentence-Selection"><a href="#Sentence-Selection" class="headerlink" title="Sentence Selection"></a>Sentence Selection</h1><p>本文在选择句子时采用了一种类似于MMR的简单贪婪算法（MMR在之前的博客中有介绍）。具体过程如下：</p>
<p>1、去掉少于8个词的句子。因为摘要不可能少于8个词。</p>
<p>2、用之前计算好的score对所有的句子进行降序排列。</p>
<p>3、迭代地将排名靠前的且不冗余的句子添加到队列中。这里的冗余定义为该句子相比进入队列的句子有更新的内容。</p>
<p>具体算法流程如下：</p>
<img src="/2016/05/10/自动文摘（八）/select.png" width="600" height="600">
<p><code>句子的选择算法几乎就是MMR，也是一种贪心的思路。不同的地方在于对冗余的定义不如MMR，MMR是用当前句子与已经在队列中的句子的相似度作为冗余判断，其实这样更加科学。</code></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>数据集用DUC2005-2007三年的数据，用两年的数据作为训练集，一年的数据作为测试集。</p>
<h2 id="Model-Setting"><a href="#Model-Setting" class="headerlink" title="Model Setting"></a>Model Setting</h2><p>CNN层：50维词向量，用gensim实现，训练过程中不更新词向量，窗口尺寸选择2，即2-gram，和ROUGE-2保持一致，句子向量维度用5-100进行试验，最终用50作为句子向量维度。</p>
<h2 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h2><p>评价指标用ROUGE-2。</p>
<h2 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h2><p>使用了之前成绩不错的MultiMR和SVR系统作为baselines，同时为了验证本文模型的有效性，构造了一个ISOLATION系统，单独考虑相关性和显著性。</p>
<h2 id="Summarization-Performance"><a href="#Summarization-Performance" class="headerlink" title="Summarization Performance"></a>Summarization Performance</h2><p>对比结果看下图：</p>
<img src="/2016/05/10/自动文摘（八）/result.png" width="600" height="600">
<p><code>整体来看本文的算法结果具有竞争性，但没有绝对竞争性。训练数据用ROUGE-2指标做了预处理分析，目标函数也是朝着ROUGE-2最大的方向，最后的评价指标也是ROUGE-2，在DUC2005和2006上很容易出现过拟合的情况，比其他结果表现好也是正常情况。整体感觉模型的效果很一般。</code></p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1604.00125v1.pdf" target="_blank" rel="external">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-07T19:02:15.000Z"><a href="/2016/05/07/自动文摘（七）/">2016-05-07</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/07/自动文摘（七）/">自动文摘（七）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>再坚持一下，就会等到黎明破晓，重见光明。</strong></p>
</blockquote>
<p>今天继续分享一篇sentence level abstractive summarization相关的paper，出自于IBM Watson，<b>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</b></p>
<p><code>本文是一篇非常优秀的paper，在seq2seq+attention的基础上融合了很多的features、trick进来，提出了多组对比的模型，并且在多种不同类型的数据集上做了评测，都证明了本文模型更加出色。从本文中得到了很多的启发。</code></p>
<p><code>（1）单纯地data-driven模型并不能很好地解决文本摘要的问题，针对文本摘要问题的特点，融合一些feature到模型之中，对模型的效果有很大的帮助。</code></p>
<p><code>（2）他山之石可以攻玉。其他领域的研究成果，小的trick都可以尝试借鉴于文本摘要问题之中，比如seq2seq+attention的技术从机器翻译中借鉴过来应用于此，比如LVT技术等等。</code></p>
<p><code>（3）文本摘要问题的解决需要解决好方方面面的问题，不仅仅是模型方面，还有数据集，还有评价指标，每个方面的进步都会是一大进步。</code></p>
<p><code>（4）deep learning技术训练出的模型泛化能力和扩展能力还有很长的路要走，对训练数据的严重依赖，导致了泛化能力和扩展能力的不足。针对特定的问题，构建特定的训练数据集，这对corpus的建设提出了更高的要求。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p> 本文将自动文摘问题当作一个<code>Seq2Seq</code>的问题，并且应用<code>Attentional Encoder-Decoder Recurrent Neural Networks</code>框架来解决这个问题，并且在两个不同的数据集上取得了超越ABS（Rush,2015）模型的结果。同时，本文还提出多种模型来研究自动文摘中的重要问题，比如对关键词进行建模，并且得出词对于文档研究起关键作用的结论。研究结果表明本文的解决方案在性能上有很大的提升，另外，还贡献了一个包括多句子文摘的数据集和基准。</p>
<p><code>本文的贡献点有三个：（1）在两个新的数据集上应用seq2seq+attention模型，并且取得了state-of-the-art的结果；（2）研究了关键词对于自动文摘所起到的关键作用，并且提出了一种新的模型；（3）提出了一个新的数据集，供研究者使用。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文研究的文本摘要问题是根据一篇长文来生成一篇短摘要或者标题，可以等同于将输入文本序列映射为一个目标文本序列，也就是seq2seq的问题。目前，解决seq2seq的问题，都是借鉴machine translation的方法。</p>
<p>但文本摘要问题和机器翻译问题有着很大的不同。</p>
<p>（1）文本摘要的问题输出长度一般都很短，并且不依赖于输入的长度。</p>
<p>（2）文本摘要的问题一般都会损失大量的信息，只保留少量的核心概念作为输出，而机器翻译则要保证信息损失最低，在单词级别上保证对齐。</p>
<p>那么机器翻译的相关技术是否会在文本摘要问题上表现同样突出呢？本文将会回答这个问题。受文本摘要与机器翻译问题的不同特点所启发，本文将超越一般的架构而提出新的模型来解决摘要问题。</p>
<p><code>本文与之前seq2seq类的paper有着一个很明显的区别，就是将摘要问题和机器翻译问题严格区别开，而不是简单地套用MT的技术来解决摘要问题，根据摘要问题的特点提出了更加合适的模型，相比于之前的研究更进了一步。</code></p>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><p>之前大量的研究都是集中于extractive摘要方法，在DUC2003和2004比赛中取得了不俗的成绩。但人类在做摘要工作时，都是采用abstractive方法，理解一篇文档然后用自己的语言转述出来。随着深度学习技术在NLP任务中的广泛使用，研究者们开始更多地研究abstractive方法，比如Rush组的工作，比如哈工大Hu的工作。</p>
<p>本文的贡献：</p>
<p>（1）在两种不同数据集上应用seq2seq+attention的模型，得到了state-of-the-art结果。</p>
<p>（2）根据摘要问题的特点提出了针对性的模型，结果更优。</p>
<p>（3）提出了一个包括多句子摘要的数据集和基准。</p>
<p><code>绝大多数的extractive方法都是unsupervised learning方法，不需要数据集来做训练，更适合搭建实用的文本摘要系统；而abstractive方法一般都是supervised learning方法，虽然在评测任务中表现更佳，但需要大量的领域数据做训练，横向扩展性并不好，数据集的内容、质量和规模都直接影响着模型的效果，目前比较难应用在实际系统中。所以，在data-driven的模型中，往往都需要配合大规模、高质量、领域相关的数据集。相比之下deep learning的方法更加简单粗暴一下，并不需要什么领域知识和特征工程，只要给定输入输出，就能拟合出一个巨大的神经网络，并且取得优于传统解决方案的结果，但太过依赖于数据，因此丧失了一般性。</code></p>
<h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1><h2 id="Encoder-Decoder-with-Attention"><a href="#Encoder-Decoder-with-Attention" class="headerlink" title="Encoder-Decoder with Attention"></a>Encoder-Decoder with Attention</h2><p>这个模型本文的基准模型，Encoder是一个双向GRU-RNN，Decoder是一个单向GRU-RNN，两个RNN的隐藏层大小相同，注意力模型应用在Encoder的hidden state上，一个softmax分类器应用在Decoder的生成器上。</p>
<p><code>基准模型是套用Bahdanau,2014在机器翻译中的方法，解决方案的思路都与之前的paper类似，并无新颖之处。</code></p>
<h2 id="Large-Vocabulary-Trick"><a href="#Large-Vocabulary-Trick" class="headerlink" title="Large Vocabulary Trick"></a>Large Vocabulary Trick</h2><p>这个模型引入了<code>large vocabulary trick(LVT)</code>技术到文本摘要问题上。本文方法中，每个mini batch中decoder的词汇表受制于encoder的词汇表，decoder词汇表中的词由一定数量的高频词构成。这个模型的思路重点解决的是由于decoder词汇表过大而造成softmax层的计算瓶颈。本模型非常适合解决文本摘要问题，因为摘要中的很多词都是来自于原文之中。</p>
<p><code>LVT是一个针对文本摘要问题的有效方法，考虑到了摘要中的大部分词都是来源于原文之中，所以将decoder的词汇表做了约束，降低了decoder词汇表规模，加速了训练过程。</code></p>
<h2 id="Vocabulary-expansion"><a href="#Vocabulary-expansion" class="headerlink" title="Vocabulary expansion"></a>Vocabulary expansion</h2><p>LVT技术很好地解决了decoder在生成词时的计算瓶颈问题，但却不能够生成新颖的有意义的词。为了解决这个问题，本文提出了一种扩展LVT词汇表的技术，将原文中所有单词的一度最邻近单词扩充到词汇表中，最邻近的单词在词向量空间中cosine相似度来计算得出。</p>
<p><code>词汇表的扩展是一项非常重要的技术，word embedding在这里起到了关键作用。用原文中单词的最邻近单词来丰富词汇表，不仅仅利用LVT加速的优势，也弥补了LVT带来的问题。</code></p>
<h2 id="Feature-rich-Encoder"><a href="#Feature-rich-Encoder" class="headerlink" title="Feature-rich Encoder"></a>Feature-rich Encoder</h2><p>文本摘要面临一个很大的挑战在于确定摘要中应该包括哪些关键概念和关键实体。因此，本文使用了一些额外的features，比如：词性，命名实体标签，单词的TF和IDF。将features融入到了word embedding上，对于原文中的每个单词构建一个融合多features的word embedding，而decoder部分，仍采用原来的word embedding。</p>
<p><code>本文的一个创新点在于融入了word feature，构建了一组新的word embedding，分别考虑了单词的词性、TF、IDF和是否为命名实体，让单词具有多个维度的意义，而这些维度上的意义对于生成摘要至关重要。本文结果的优秀表现再一次印证了简单粗暴的纯data driven方法比不上同时考虑feature的方法。后面的研究可以根据本文的思路进行进一步地改进，相信会取得更大的突破。</code></p>
<h2 id="Switching-Generator-Pointer"><a href="#Switching-Generator-Pointer" class="headerlink" title="Switching Generator/Pointer"></a>Switching Generator/Pointer</h2><p>文本摘要中经常预见这样的问题，一些关键词出现很少但却很重要，由于模型基于word embedding，对低频词的处理并不友好，所以本文提出了一种decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中。pointer机制在解决低频词时鲁棒性比较强，因为使用了encoder中低频词的隐藏层表示作为输入，是一个上下文相关的表示，而仅仅是一个词向量。</p>
<img src="/2016/05/07/自动文摘（七）/Pointer.png" width="600" height="800">
<p><code>Pointer机制与某篇paper中的Copy机制有异曲同工之妙，都是用来解决OOV问题的，本文的最关键的是如何计算开关状态是generator的概率，这一层的计算关系到当前time step是采用generator模式还是pointer模式。</code></p>
<h2 id="Hierarchical-Encoder-with-Hieratchical-Attention"><a href="#Hierarchical-Encoder-with-Hieratchical-Attention" class="headerlink" title="Hierarchical Encoder with Hieratchical Attention"></a>Hierarchical Encoder with Hieratchical Attention</h2><p>数据集中的原文一般都会很长，原文中的关键词和关键句子对于形成摘要都很重要，这个模型使用两个双向RNN来捕捉这两个层次的重要性，一个是word-level，一个是sentence-level，为了区别与Li的工作，本文在两个层次上都使用注意力模型。注意力权重如下：</p>
<img src="/2016/05/07/自动文摘（七）/formula1.png" width="300" height="300">
<p>本文模型示意图如下：</p>
<img src="/2016/05/07/自动文摘（七）/Attention.png" width="600" height="800">
<p><code>注意力机制的本质是一组decoder与encoder之间相关联的权重，本文在两个层次上重新定义了attention weight，既考虑了某个encoder中每个word对于decoder的重要性，也考虑了该word所在sentence对于decoder的重要性。</code></p>
<h1 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h1><h2 id="Gigaword-Corpus"><a href="#Gigaword-Corpus" class="headerlink" title="Gigaword Corpus"></a>Gigaword Corpus</h2><p>为了作对比，本文采用了Rush文章中的Gigaword数据集和他的开源代码来处理数据，形成了380万训练样本和约40万验证样本和测试样本，本文随机选取2000组样本作为验证和测试集来测试本文模型的性能，为了更加公平地对比，本文使用了Rush采用的测试集来对比。</p>
<p>初始词向量的生成是用Word2Vec，但在训练的过程中会更新词向量。训练的参数设置也都采用一般的训练设置。</p>
<p>在Decoder阶段，采用大小为5的<code>beam search</code>来生成摘要，并且约束摘要长度不大于30个单词。</p>
<p>评价指标方面，采用full-length Rouge召回率，然而该指标更加青睐于长摘要，所以在比较两个生成不同长度摘要的系统时并不公平，用full-length F1来评价更加合理。</p>
<p>对比实验共有以下几组：</p>
<p>（1）words-1sent：baseline模型，对应之前的Encoder-Decoder with Attention模型。1sent表示模型的输入是原文的第一句话。</p>
<p>（2）words-lvt2k-1sent：对应之前的Large Vocabulary Trick模型。lvt2k表示decoder的词汇表上限是2000。</p>
<p>（3）words-lvt2k-(2|5)sent：与第二组实验采用相同的模型，只是输入采用了原文的前两句话和前五句话。</p>
<p>（4）words-lvt2k-2sent-exp：对应之前的Vocabulary expansion模型。</p>
<p>（5）words-lvt2k-2sent-hieratt：对应之前的Hierarchical Encoder with Hieratchical Attention模型。</p>
<p>（6）big-words-lvt2k-(1|2)sent：模型与第二组实验相同，但将embedding size和hidden state size增大到了200和400。</p>
<p>（7）big-feats-lvt2k-2sent：对应之前的Feature-rich Encoder模型。</p>
<p>（8）feats-lvt2k-2sent-ptr：对应之前的Switching Generator/Pointer模型。</p>
<p>实验结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result1.png" width="600" height="800">
<p>从表中清晰地看到switching generator/pointer模型在各个指标上都是最好的模型，本文的模型在Rush测试集中的结果都优于Rush的ABS+模型。</p>
<p><code>Gigaword由于其数据量大的特点，常被用于文本摘要任务中作为训练数据。本文的训练、生成参数都沿用了传统的方法，评价指标选择了更合适的F1，共设计了8大组实验，从方方面面对比了各个模型之间的优劣，从多个角度说明了本文模型比前人的模型更加优秀。</code></p>
<p><code>数据集对于deep learning是至关重要的，构建一个合适的数据集是一个非常有意义的工作。哈工大之前有一个工作就是构建了微博摘要的数据集，方便了研究中文文本摘要的研究者。</code></p>
<h2 id="DUC-Corpus"><a href="#DUC-Corpus" class="headerlink" title="DUC Corpus"></a>DUC Corpus</h2><p>DUC2003作为模型的验证集，DUC2004作为对比测试的数据集，模型的训练都是通过Gigaword来做，这里主要是为了对比本文模型和Rush的ABS和ABS+模型，结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result2.png" width="600" height="800">
<p>在DUC2003中big-words-lvt2k-1sent表现更加突出，所以用该模型来与其他系统进行对比，结果明显优于其他系统。</p>
<p><code>本文模型相比于其他系统的优势在DUC数据集中并不如Gigaword数据集上更加明显。因为大家的模型都是采用Gigaword来做的，模型都非常好地拟合了Gigaword数据集。从这个结论中也可以看出，deep learning技术对于数据集规模、质量和类型的依赖，并不能很好地泛化到其他数据内容中。</code></p>
<h2 id="CNN-DailyMail-Corpus"><a href="#CNN-DailyMail-Corpus" class="headerlink" title="CNN/DailyMail Corpus"></a>CNN/DailyMail Corpus</h2><p>现有的abstractive摘要系统都是单句摘要，本节实验将要证明本文的模型同样在多句摘要任务中会有更好的表现。</p>
<p>实验结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result3.png" width="600" height="800">
<p>从表中结果可以看出，switching generator/pointer模型更加优秀。</p>
<p><code>本文的一大贡献在于构建了CNN/DailyMail文本摘要数据集，用来评测多句摘要的任务，为今后大量的相关工作提供了数据保障。</code></p>
<h1 id="Qualitative-Analysis"><a href="#Qualitative-Analysis" class="headerlink" title="Qualitative Analysis"></a>Qualitative Analysis</h1><p>本文的模型在一些数据的处理会理解错原文的意思，生成一些“错误”的摘要。另外，Switching Generator/Pointer模型不仅仅在处理命名实体上有优势，而且在处理词组上表现也非常好。未来的工作中，将会对该模型进行更多的实验。效果见下图：</p>
<img src="/2016/05/07/自动文摘（七）/result4.png" width="600" height="800">
<p><code>本文模型相对于Rush的模型有了更进一步的效果，但对于文本摘要问题来说，并没有本质上的提升，也会经常出现这样或者那样的错误。指标上的领先是一种进步，但与评价指标太过死板也有关系，现有的评价指标很难从语义这个层次上来评价结果。所以，文本摘要问题的解决需要解决方方面面的问题，比如数据集，比如评价指标，比如模型，任何一个方面的突破都会带来文本摘要问题的突破。</code></p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>本文提出的模型相比于前人的模型有更好的效果，并且构建了一个新的摘要数据集，可以满足多句摘要的任务。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1602.06023" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and
Beyond</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-30T23:30:43.000Z"><a href="/2016/04/30/自动文摘（六）/">2016-04-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/30/自动文摘（六）/">自动文摘（六）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>万事开头难，其实之后的事情可能会更难，但开好了头，就会有充足的信心来面对后面的困难。</strong></p>
</blockquote>
<p>记得Andrew Ng在一个采访中曾经说过：“当我和研究人员，或是想创业的人交谈时，我告诉他们如果你不断地阅读论文，每周认真研究六篇论文，坚持两年。然后，你会学到很多东西。这是对你长期发展一个极好的投资。”，这个投资将会是一个回报非常高的投资。其实，不仅仅是坚持读论文，当你将任何一件不起眼但却有一些意义的事情坚持做很长的时间都会得到不错的回报，比如健身，比如跑步，比如写博客，比如摄影，每一个这样的习惯看起来都不会立刻产生很明显的效果，但坚持久了对我们的生活和工作都会带来一个质的飞跃。</p>
<p>上周写了篇paper的读书笔记，也是对自己读了那篇paper的一些思考的提炼，本周将会带来对两篇paper的思考。其中一篇是<b>A Neural Attention Model for Abstractive Sentence Summarization</b>，另一篇是<b>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</b>，两篇文章都出自于<a href="http://harvardnlp.github.io/" target="_blank" rel="external">Harvard NLP</a>组，两篇是姊妹篇，第二篇是第一篇的升级版，所以要结合着读，对比着分析。</p>
<p><code>世上没有什么所谓的银弹，每种方法存在都有其存在的意义。第一篇paper尝试将seq2seq+attention应用在summarization任务上，但并未取得比较令人满意的结果，反而增加了一些人工特征之后，才得到了很大的提升，虽然第二篇模型依旧是一个data-driven的模型，但我想如果给其添加上人工特征也会得到更好的效果。综合多种方法的优点来解决一个问题才是王道，而不是一味地、粗暴地套用某个范式，某个框架。</code></p>
<p><code>两篇文章从同一个角度入手，采用了不同难度的模型，非常好地解决了这个问题。联想到上周看的paper，他所采用的是多层lstm作为encoder和decoder，但数据集使用的并不相同，所以并不知道与本周的两篇paper哪个效果更好。但这也给出了一种发paper的思路，多去尝试一些encoder和decoder模型，不断地组合和对比，一定会有不错的发现。但这样的解决方案对于提升层次上没有太多溢出，因为大家都是照着模板去做，并没有真正地更深地理解到这个问题的本质。</code></p>
<p><code>一个系统的构建需要处理好方方面面的细节，比如数据的预处理，比如评测的实现，比如模型的参数调优，每个方面想要做好做精都是一门学问。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><h2 id="Paper-1"><a href="#Paper-1" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>本文提出了一种<code>data-driven</code>的方法来做句子摘要，使用了一种基于局部注意力模型（<code>local attention-based model</code>），在给定输入句子的情况下，生成摘要的每个词。模型结构非常简单，可以套用流行的<code>end2end</code>框架来训练，并且很容易扩展到大型训练数据集上。模型在DUC-2004任务中效果优于几个不错的baselines。</p>
<h2 id="Paper-2"><a href="#Paper-2" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文使用一种conditional RNN来生成摘要，条件是卷积注意力模型（<code>convolutional attention-based encoder</code>），用来确保每一步生成词的时候都可以聚焦到合适的输入上。模型仅仅依赖于学习到的features，并且很容易在大规模数据上进行end2end式地训练，并且在Gigaword语料上和DUC-2004任务中取得了更好的效果。</p>
<p><code>两篇paper的模型框架都是seq2seq+attention，最大的区别在于选择encoder和decoder的模型，第一篇的模型偏容易一些，第二篇用了rnn来做。seq2seq或者说end2end现在火的不得了，最初在机器翻译开始使用，后面推广到多模态学习，对话生成，自动问答，文本摘要等等诸多领域。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>文本摘要有几种类型的任务，本文属于<code>headlines generation</code>，输入的是一段话，输出的是一句话或者一个标题。</p>
<h2 id="Paper-1-1"><a href="#Paper-1-1" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>受最近机器翻译中seq2seq技术的启发，本文将<code>Neural Language Model</code>和带有上下文的encoder结合起来，其中encoder与Bahdanau（<b>Neural machine translation by jointly learning to align and translate</b>）的attention-based encoder一样。encoder和decoder在句子摘要任务中共同训练。另外，decoder中也使用了beam search进行摘要生成。本文的方法简称ABS（<code>Attention-Based Summarization</code>），可以轻易扩展到大规模数据集进行训练，而且可以在任何document-summary对中进行使用。本文采用Gigaword语料集进行训练，包括大约400万篇新闻文章。为了检验模型的效果，与多种类型的文摘系统进行了对比，并且在DUC-2004任务上获得了最高分。</p>
<h2 id="Paper-2-1"><a href="#Paper-2-1" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文的decoder是一个<code>RNN LM</code>，生成摘要依赖的条件是encoder的输出，encoder会计算输入中每个词的分数，这个分数可以理解为对输入作软对齐（<code>soft alignment</code>），也就是说decoder在生成下一个单词时需要注意输入中的哪些单词。encoder和decoder要在一个sentence-summary数据集中进行共同训练。本文的模型可以看作第一篇ABS模型的扩展，ABS模型中decoder是用FNN LM，而本文使用RNN，encoder部分本文更加复杂，将输入单词的位置信息考虑在内，并且使用了卷积网络来编码输入单词。本文模型效果优于第一篇paper。</p>
<p><code>两篇paper都是seq2seq在sentence-level abstractive summarization任务中早期的尝试，给文本摘要方法带来了新鲜血液，第一篇paper中encoder和decoder都用了比较简单的模型，但已经得到了优于传统方法的结果，再一次地证明了deep learning在解决问题上的优势，第二篇paper升级了encoder和decoder，考虑了更复杂的细节，得到了更好的效果，相信后面会有大量的paper套用seq2seq+attention，再配合一些其他的技术来提升模型的效果，但整体的思路基本已固定下来，如果想要更大的突破，可能还需要提出另外一种框架来解决问题。</code></p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>这部分内容是初入门径的研究者最喜欢的工作，尤其是这个领域中最新研究的paper还没有出survey的情况下，大家想最快地了解这个领域中新技术的应用情况，读高水平paper中的相关工作是最有效的。</p>
<h2 id="Paper-1-2"><a href="#Paper-1-2" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>文本摘要任务在sentence这个level可以等同于headlines generation，某种程度上与paraphrase相近。seq2seq于2014年在机器翻译领域中提出并流行开来，之前的研究大多都是基于extractive的思路，借助一些人工features来提升效果。seq2seq的意义在于完全基于数据本身，从数据中学习feature出来，并且得到了更好的效果。本文的方法比较简单，decoder也只用了NNLM（2003年由Bengio提出），而seq2seq在机器翻译中应用时都采用的是RNNLM，所以在Future Work中作者会用RNNLM，于是就有了第二篇paper。</p>
<h2 id="Paper-2-2"><a href="#Paper-2-2" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>由于都是一个组出的paper，还有共同的作者，这个部分写的差不多，只是多提了第一篇paper做的工作。</p>
<p><code>都说 读书破万卷，下笔如有神。在做一个领域的研究之前，免不了读大量相关的paper来做一些积累，related work这个部分就是大家写的小型survey，经常会提到一些该领域最经典的paper。感觉Rush他们组应该是比较新的NLP研究力量，将一个新的技术用在了自动文摘领域中，攒了两篇paper，也是数量上的一种积累。不过他们share了paper相关的code，用Torch来写模型部分，用python作数据处理。组里也包括那位将CNN用在sentence classification中的Yoon Kim，相信他们日后会有更多更好的成果。</code></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>本节是用数学语言定义句子摘要问题，两篇文章解决的问题相同。给定一个输入句子，目标是生成一个压缩版的摘要。句子级别的摘要问题可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula1.png" width="300" height="400">
<p>x表示输入句子，y表示生成的摘要句子集合，定义一个系统是abstractive的，就是从生成句子集合中找到score最大的那一个。而extractive摘要系统可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula2.png" width="300" height="400">
<p>sentence compression系统可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula3.png" width="300" height="400">
<p>然而abstractive summarization存在一个更加困难的生成过程。</p>
<p><code>用了一个简单的数学公式将问题描述地非常清楚，包括一些细节，比如输入长度大于输出长度，输出长度为固定值，输入输出拥有相同的词汇表等等。从数学公式来看score函数的定义很重要，考虑的参数类型不同会有不同的score，也就是不同的模型，明显看得出abstractive要远难于extractive和sentence compression。</code></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>模型部分是paper的重头戏，分为Objective，Encoder，Decoder，Generation，Training五个子部分来讨论。</p>
<h2 id="Paper-1-3"><a href="#Paper-1-3" class="headerlink" title="Paper 1"></a>Paper 1</h2><h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><p>目标函数是Negative Log-Likelihood（NLL），decoder中生成摘要单词的条件是encoder的输出和当前生成词的窗口词向量，具体如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula4.png" width="300" height="400">
<p>这里当前生成词的窗口词向量由下式表示：</p>
<img src="/2016/04/30/自动文摘（六）/formula5.png" width="100" height="100">
<p>其实也就是NNLM中的N-gram，用来预测下一个词。目标函数表示为：</p>
<img src="/2016/04/30/自动文摘（六）/formula6.png" width="300" height="400">
<p>对于i&lt;1的情况，在句子前padding几个开始符号。接下来建模的部分就是研究如何表达条件概率。</p>
<p><code>目标函数用生成词的条件概率的对数来表示是NLP中非常常用的做法。不同的模型都在研究如何表示条件，比如encoder的表示，encoder输出的表示，decoder中当前词前序词的表示等等。</code></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>本文一共提出了三种encoder模型。</p>
<h4 id="Bag-of-Words-Encoder"><a href="#Bag-of-Words-Encoder" class="headerlink" title="Bag-of-Words Encoder"></a>Bag-of-Words Encoder</h4><p>词袋模型是最简单的一个模型，将输入的句子用词袋模型降维到H，生成一个word embedding层。模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/bow.png" width="300" height="400">
<p><code>词袋模型并不会考虑词序的关系，效果并不会太好，但是作为paper中的一个baseline模型会有很好的对比结果。</code></p>
<h4 id="Convolutional-Encoder"><a href="#Convolutional-Encoder" class="headerlink" title="Convolutional Encoder"></a>Convolutional Encoder</h4><p>卷积模型是一个深度网络模型，可以很好地捕捉输入的特征。模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/CNN.png" width="300" height="400">
<p>其中矩阵F是输入句子的word embedding矩阵，Q包括了一系列过滤层，并且采用了最大池化技术来处理。</p>
<p><code>CNN通过结合word embedding将句子表示成一个matrix，通过不同尺寸的卷积核来filter出句子中的feature，本质上和N-gram一样，N-gram的N就是卷积核的尺寸，构建出多种feature maps，然后max pooling，然后filter，然后pooling，最终采用一个MLP得出结果。</code></p>
<h4 id="Attention-Based-Encoder"><a href="#Attention-Based-Encoder" class="headerlink" title="Attention-Based Encoder"></a>Attention-Based Encoder</h4><p>虽然卷积模型比词袋模型更能捕捉句子的特征，却同样需要对整个句子做表示，机器翻译领域在解决相同问题时采用了注意力模型来构建context，然后基于生成的context来构建representation。本文采用一种类似于词袋模型的注意力模型，模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/attention.png" width="300" height="400">
<p>其中矩阵G是context的word embedding矩阵，P是一个权重矩阵，权重连接着输入word embedding和context embedding，Q是一个光滑窗口，流程如下图：</p>
<img src="/2016/04/30/自动文摘（六）/attention2.png" width="300" height="400">
<p><code>本文的注意力模型可以视作将词袋模型中的P向量用一个待学习的soft alignment来替换了。</code></p>
<p><code>三种encoder模型给出了input sentence的表示，第三种还给出了summary和input之间的关系，encoder的输出将作为decoder的输入，来生成summary。</code></p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder的本质就是一个神经网络语言模型，本文用了2003年Bengio提出的NNLM，模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/NNLM.png" width="300" height="400">
<p>Bengio的模型是一个FNN（Feed-Forward Neural Network），通过上文（当前词的前N个词）来预测当前词，流程如下图：</p>
<img src="/2016/04/30/自动文摘（六）/NNLM2.png" width="300" height="400">
<p>待求的参数是word embedding矩阵E，输入层到隐藏层的权重矩阵U，隐藏层到decoder输出层的权重矩阵V，encoder输出层到decoder输出层的权重矩阵W。</p>
<p><code>NNLM是一个经典的语言模型，本质上就是一个神经网络多分类器，文中也提到可以考虑用RNNLM来作decoder，也就有了第二篇paper的模型。</code></p>
<h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>一般的语言模型都是基于上下文生成概率最高的一个词，但对于生成摘要句子来说还不够。通常的做法是用一种搜索算法在一定的可行域之内找到几组可行的解。本文采用<code>beam search</code>，也是之前机器翻译领域生成翻译结果时常用的算法，算法描述如下：</p>
<img src="/2016/04/30/自动文摘（六）/beamsearch.png" width="300" height="400">
<p><code>给定一个beam size K，在生成每一个summary word时，都保留概率最大的K个词，从生成第二个词开始，计算所有路径的概率，只保留概率最大的前K个分枝，裁剪掉剩余的分枝，继续生成第三个词，依次进行下去，直到生成的词是EOS或者达到最大句子长度限制。最后得到的结果是K个最好的sentence summary。</code></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>本文采用mini batch SGD算法对训练集进行训练，使得NLL最小。</p>
<p><code>因为在生成summary时并没有什么约束条件，所以本方法可以拓展到任意input-output pairs中使用。</code></p>
<h3 id="ABS"><a href="#ABS" class="headerlink" title="ABS+"></a>ABS+</h3><p>作者提出了一个纯数据驱动的模型之后，又提出了一个abstractive与extractive融合的模型，在ABS模型的基础上增加了feature function，修改了score function，也就是结果对比中的<code>ABS+</code>模型。</p>
<h2 id="Paper-2-3"><a href="#Paper-2-3" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文模型简称为RAS（<code>Recurrent Attentive Summarizer</code>）</p>
<h3 id="Objective-1"><a href="#Objective-1" class="headerlink" title="Objective"></a>Objective</h3><p>目标函数如下：</p>
<img src="/2016/04/30/自动文摘（六）/NLL2.png" width="300" height="400">
<p>两篇paper都是采用NLL，但不同的是第二篇paper目标函数条件概率中的条件与第一篇不同，本文采用decoder的所有上文，而不是一个窗口内的上文。</p>
<h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><p>encoder的输出是decoder的输入，对于每一个time step，encoder都需要给出一个context vector，本文encoder的重点在于如何计算时间相关的context。</p>
<p>输入句子每个词最终的embedding是各词的embedding与各词位置的embedding之和，经过一层卷积处理得到aggregate vector：</p>
<img src="/2016/04/30/自动文摘（六）/formula21.png" width="300" height="400">
<p>根据<code>aggregate vector</code>计算context（encoder的输出）：</p>
<img src="/2016/04/30/自动文摘（六）/formula22.png" width="300" height="400">
<p>其中权重由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula23.png" width="300" height="400">
<p><code>Rush组的paper有一个特点，喜欢用CNN多一些，包括那位用CNN做句子分类的童鞋。可能的原因是，Rush是Facebook AI Research的研究人员，Lecun是Leader，所以他们对CNN的理解也更深一些，在model中使用的也就更多一些。</code></p>
<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder的部分是一个RNNLM，这里的RNN Hidden Layer使用的是LSTM单元。decoder的输出由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula24.png" width="300" height="400">
<p>其中c(t)是encoder的输出，h(t)是RNN隐藏层，由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula25.png" width="300" height="400">
<p>这里隐藏层的单元有两种思路，一种是常规的Elman RNN，一种是LSTM。</p>
<p><code>RNNLM的Hidden Unit可以不用LSTM或者GRU这么复杂，普通的隐藏层Elman RNN可以解决问题，采用Truncate-BPTT对RNN进行训练（详见Tomas Mikolov的PhD Thesis）。况且LSTM和GRU会带来更多的参数，造成overfit。</code></p>
<h3 id="Generation-1"><a href="#Generation-1" class="headerlink" title="Generation"></a>Generation</h3><p>生成过程中也采用<code>beam search</code>算法进行summary生成。</p>
<h3 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h3><p>给定一个训练集，包括大量的sentence-summary pairs，用SGD将NLL函数最小化得到最优的参数集，参数包含encoder和decoder两个部分的参数。</p>
<p><code>SGD是一种常用的优化算法，在解决NLP问题中非常有效，其中最常见的mini batch训练方法。</code></p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="Paper-1-4"><a href="#Paper-1-4" class="headerlink" title="Paper 1"></a>Paper 1</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>本文采用经过处理的约400万Gigaword数据集作为训练集和验证集，在DUC2004数据集上进行评测，评测使用ROUGE方法。</p>
<p><code>DUC的比赛经常会包括文本摘要，所以常常用来比较每个模型或系统的优劣。</code></p>
<h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>1、PREFIX，这个baseline是取输入的前75个字符作为headline。</p>
<p>2、TOPIARY。</p>
<p>3、COMPRESS。</p>
<p>4、IR。</p>
<p>5、W&amp;L。</p>
<p>6、MOSES+。</p>
<p><code>baselines选择了几组非常有代表性的系统。</code></p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>本文的程序用Torch实现，并且开源在Github上，处理1000个mini batch大概用时160s，最好的验证集参数出现在第15个epoch。</p>
<p><code>Torch是一个使用率非常高的开源工具，尤其是在研究领域。相比于Theano的难以调试，Torch具有非常简单、易用、灵活、易调试的特点。</code></p>
<h2 id="Paper-2-4"><a href="#Paper-2-4" class="headerlink" title="Paper 2"></a>Paper 2</h2><h3 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a>Dataset</h3><p>与第一篇相同的训练集和处理方法，同样使用DUC2004作为评测数据，ROUGE作为评测方法。</p>
<h3 id="Baselines-1"><a href="#Baselines-1" class="headerlink" title="Baselines"></a>Baselines</h3><p>1、ABS（第一篇paper中的方法）<br>2、ABS+（第一篇paper中的方法）</p>
<h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>同样使用Torch开发，在训练时用摘要的混乱度（<code>perplexity</code>）作为评价指标控制训练过程。</p>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="Paper-1-5"><a href="#Paper-1-5" class="headerlink" title="Paper 1"></a>Paper 1</h2><img src="/2016/04/30/自动文摘（六）/result1.png" width="300" height="400">
<p>分别在DUC2004和Gigaword数据集上进行了对比，本文的ABS模型在DUC2004上评测结果相比于最好的baseline MOSES+并不如意，MOSES+是一个基于短语的统计机器翻译系统（Koehn，2007），在Gigaword训练集上比MOSES+好一些。但增加了人工feature的ABS+模型比ABS模型和MOSES+系统表现好了非常多。</p>
<img src="/2016/04/30/自动文摘（六）/result11.png" width="300" height="400">
<p>5种不同的模型在混乱度这个指标上比较，ABS具有明显的优势。</p>
<p><code>ABS模型实际上的效果并不理想，所以本文作者又提出了一种所谓的ABS+模型，将人工feature融合到了ABS模型中，得到了不错的效果。如果只看这一篇paper，可能会觉得不理想的原因是seq2seq在自动文摘中的效果一般，但看过第二篇paper之后，就会明白是因为本文的模型太过简单，第二篇paper也就有了意义。从另一个角度来看，纯粹的data-driven方法如果配合上一些extractive的方法会得到更好的结果，这点对于实际系统的开发非常有意义。</code></p>
<h2 id="Paper-2-5"><a href="#Paper-2-5" class="headerlink" title="Paper 2"></a>Paper 2</h2><img src="/2016/04/30/自动文摘（六）/result21.png" width="300" height="400">
<p>在Gigaword数据集上对比各个模型，RAS-Elman模型表现最好，说明了seq2seq相比于传统的文摘系统和算法，可以更好地解决问题，又一次证明了deep learning的强大。同时也验证了普通的RNN不见得比LSTM活着GRU表现差，尤其是当序列长度不是特别长的情况。</p>
<img src="/2016/04/30/自动文摘（六）/result22.png" width="300" height="400">
<p>在DUC2004数据集上对比各个模型，得到了相同的结论。</p>
<img src="/2016/04/30/自动文摘（六）/result23.png" width="300" height="400">
<p>5种不同的模型在混乱度这个指标上比较，本文算法RAS-Elman具有明显的优势。</p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a> <b>Proceedings of EMNLP 2015</b></p>
<p>[2] <a href="http://harvardnlp.github.io/papers/naacl16_summary.pdf" target="_blank" rel="external">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</a> <b>Proceedings of NAACL 2016</b></p>
<p>[3] <a href="https://github.com/harvardnlp/NAMAS" target="_blank" rel="external">ABS Torch Code</a></p>
<p>[4] <a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">Seq2Seq Torch Code</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/" class="alignleft prev">上一页</a>
  
  
    <a href="/page/3/" class="alignright next">下一页</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>9</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>25</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>7</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>1</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>13</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 RSarXiv
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>